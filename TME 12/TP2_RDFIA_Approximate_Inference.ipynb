{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TP2_RDFIA_Approximate_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "clhZGFZ2wOxg",
        "ZCp4ytXhV6IU",
        "sNGfQhFqtknu",
        "DsTozn8kQccU",
        "qgNFac420Nh_",
        "_g0QmYJ7WJ8p",
        "s0rMZBLxbHY9",
        "3i_scDA7bC1c"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfDd0DKVtknP"
      },
      "source": [
        "# TP 2: Approximate Inference in Classification\n",
        "\n",
        "In classification taks, even for a mere Logistic Regression, we don't have access to a closed form of the posterior $p(\\pmb{w} \\vert \\mathcal{D})$. Unlike in Linear regression, the likelihood isn't conjugated to the Gaussian prior anymore. We ill need to approximate this posterior.\n",
        "\n",
        "During this session, we will explore and compare approximate inference approaches on 2D binary classification datasets. Studied approaches include Laplacian approximation, variational inference with mean-field approximation and Monte Carlo dropout.\n",
        "\n",
        "**Goal**: Take hand on approximate inference methods and understand how they works on linear and non-linear 2D datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clhZGFZ2wOxg"
      },
      "source": [
        "### All Imports and Useful Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Cv7C3O7qaw"
      },
      "source": [
        "Here we are going to install and import everything we are going to need for this tutorial. \r\n",
        "\r\n",
        "**Note**: *You can double-click the title of the collapsed cells (as the ones below) to expand them and read their content.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-kN2Wxbv2Ui",
        "cellView": "form"
      },
      "source": [
        "#@title Import libs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "import torch.distributions as dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMDnycZQPMpL",
        "cellView": "form"
      },
      "source": [
        "#@title Useful plot function \n",
        "def plot_decision_boundary(model, X, Y, epoch, accuracy, model_type='classic', \n",
        "                           nsamples=100, posterior=None, tloc=(-4,-7), \n",
        "                           nbh=2, cmap='RdBu'):    \n",
        "    \"\"\" Plot and show learning process in classification \"\"\"\n",
        "    h = 0.02*nbh\n",
        "    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
        "    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min*2, x_max*2, h),\n",
        "                         np.arange(y_min*2, y_max*2, h))\n",
        "    \n",
        "    test_tensor = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).type(torch.FloatTensor)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      if model_type=='classic':\n",
        "          pred = model(test_tensor)\n",
        "      elif model_type=='laplace':\n",
        "          #Save original mean weight\n",
        "          original_weight = model.state_dict()['fc.weight'].detach().clone()\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              state_dict = model.state_dict()\n",
        "              state_dict['fc.weight'] = torch.from_numpy(posterior[i].reshape(1,2))\n",
        "              model.load_state_dict(state_dict)\n",
        "              outputs[i] = net(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "          state_dict['fc.weight'] = original_weight\n",
        "          model.load_state_dict(state_dict)\n",
        "      elif model_type=='vi':\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              outputs[i] = model(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "      elif model_type=='mcdropout':\n",
        "          model.eval()\n",
        "          model.training = True\n",
        "          outputs = torch.zeros(nsamples, test_tensor.shape[0], 1)\n",
        "          for i in range(nsamples):\n",
        "              outputs[i] = model(test_tensor)\n",
        "          pred = outputs.mean(0).squeeze()\n",
        "    Z = pred.reshape(xx.shape).detach().numpy()\n",
        "\n",
        "    plt.cla()\n",
        "    ax.set_title('Classification Analysis')\n",
        "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n",
        "    ax.contour(xx, yy, Z, colors='k', linestyles=':', linewidths=0.7)\n",
        "    ax.scatter(X[:,0], X[:,1], c=Y, cmap='Paired_r', edgecolors='k');\n",
        "    ax.text(tloc[0], tloc[1], f'Epoch = {epoch+1}, Accuracy = {accuracy:.2%}', fontdict={'size': 12, 'fontweight': 'bold'})\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAk_gzoRtknr"
      },
      "source": [
        "## Part I: Bayesian Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4aNuqE77jsS"
      },
      "source": [
        "In linear regression, model prediction is of the continuous form $f(\\pmb{x})=\\pmb{w}^T\\pmb{x}+b$.\r\n",
        "\r\n",
        "For classification, we wish to predict discrete class labels $\\mathcal{C}_k$ to a sample $\\pmb{x}$. \r\n",
        "For simplicity, let's consider here binary classification:\r\n",
        "$$f(\\pmb{x}) = \\sigma(\\pmb{w}^T\\pmb{x} + b)$$\r\n",
        "where $\\sigma(t)= \\frac{1}{1+e^t}$ is the sigmoid function.\r\n",
        "\r\n",
        "As in linear regression, we define a Gaussian prior: \r\n",
        "$$ p(\\pmb{w}) = \\mathcal{N}(\\pmb{w}; \\pmb{\\mu}_0, \\pmb{\\Sigma}_0^2) $$\r\n",
        "Unfortunately, the posterior distribution isn't tractable as the likelihood isn't conjugate to the prior anymore.\r\n",
        "\r\n",
        "We will explore in the following different methods to obtain an estimate of the posterior distribution and hence the predictive distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCp4ytXhV6IU"
      },
      "source": [
        "### I.0 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-QrdljAUG5E",
        "cellView": "form"
      },
      "source": [
        "#@title Hyperparameters for model and approximate inference { form-width: \"30%\" }\n",
        "\n",
        "WEIGHT_DECAY = 5e-2 #@param\n",
        "NB_SAMPLES = 400 #@param\n",
        "TEXT_LOCATION = (-5,-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPI60gDOtkns"
      },
      "source": [
        "# Load linear dataset\n",
        "X, y = make_blobs(n_samples=NB_SAMPLES, centers=[(-2,-2),(2,2)], cluster_std=0.80, n_features=2)\n",
        "X, y = torch.from_numpy(X), torch.from_numpy(y)\n",
        "X, y = X.type(torch.float), y.type(torch.float)\n",
        "torch_train_dataset = data.TensorDataset(X,y) # create your datset\n",
        "train_dataloader = data.DataLoader(torch_train_dataset, batch_size=len(torch_train_dataset))\n",
        "\n",
        "# Visualize dataset\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired_r', edgecolors='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNGfQhFqtknu"
      },
      "source": [
        "### I.1 Maximum-A-Posteriori Estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CE5ewMVdt9"
      },
      "source": [
        "\r\n",
        "In this \"baseline\", we reduce our posterior distribution $p(\\pmb{w} | \\mathcal{D})$ to a point estimate $\\pmb{w}_{MAP}$. For a new sample $\\pmb{x^*}$, the predictive distribution can then be approximated by\r\n",
        "$$ p(\\mathbf{y} = 1|\\pmb{x^*},\\mathcal{D}) = \\int p(\\mathbf{y} =1 |\\pmb{x},\\pmb{w})p(\\pmb{w} | \\mathcal{D})d\\pmb{w} \\approx p(y =1 |\\pmb{x},\\pmb{w}_{\\textrm{MAP}}).$$\r\n",
        "This approximation is called the **plug-in approximation**.\r\n",
        "\r\n",
        "The point estimate corresponds to the Maximum-A-Posteriori minimum given by:\r\n",
        "$$ \\pmb{w}_{\\textrm{MAP}} = arg \\max_{\\pmb{w}} p(\\pmb{w} \\vert \\mathcal{D}) = arg \\max_{\\pmb{w}} p(\\mathcal{D} \\vert \\pmb{w})p(\\pmb{w}) = arg \\max_{\\pmb{w}} \\prod_{n=1}^N p(y_n \\vert \\pmb{x}_n, \\pmb{w})p(\\pmb{w}) $$\r\n",
        "Looking for the maximum solution of previous equation is equivalent to the minimum solution of $- \\log p(\\pmb{w} \\vert \\mathcal{D})$. In case of a Gaussian prior, it can further be derived as:\r\n",
        "$$ \\pmb{w}_{\\textrm{MAP}} = arg \\min_{\\pmb{w}} \\sum_{n=1}^N \\big ( -y_n \\log \\sigma(\\pmb{w}^T \\pmb{x}_n + b) - (1-y_n) \\log (1 - \\sigma(\\pmb{w}^T \\pmb{x}_n + b)) + \\frac{1}{2 \\Sigma_0^2} \\vert \\vert \\pmb{w} \\vert \\vert_2^2 \\big ) $$\r\n",
        "\r\n",
        "Note that:\r\n",
        "- This actually correspond to the minimum given by the standard **cross-entropy** loss in classification with a weight decay regularization\r\n",
        "- Unlike in linear regression, $\\pmb{w}_{MAP}$ **cannot be computed analytically**\r\n",
        "- But we can use optimization methods to compute it, e.g. **stochastic gradient descent**\r\n",
        "- Nevertheless, we only obtain a **point-wise estimate**, and not a full distribution over parameters $\\pmb{w}$\r\n",
        "\r\n",
        "\r\n",
        "Consequently, **the objective is simply to implement and train a Logistic Regression model** with Pytorch and then compute $p(\\mathbf{y} = 1|\\pmb{x}^*,\\mathcal{D})$ on a new sample $\\pmb{x}^*$ as in a deterministic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_I-DiH3tknv"
      },
      "source": [
        "#@title **[CODING TASK]** Implement Logistic Regression\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  \"\"\" A Logistic Regression Model with sigmoid output in Pytorch\"\"\"\n",
        "  def __init__(self, input_size):\n",
        "    super().__init__()\n",
        "    # ============ YOUR CODE HERE ============\n",
        "\n",
        "  def forward(self, x):\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    # Don't forget to apply the sigmoid function when returning the output\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx-vyfGdtknw"
      },
      "source": [
        "#@title **[CODING TASK]** Train a Logistic Regression model with stochastic gradient descent for 20 epochs.\n",
        "\n",
        "net = LogisticRegression(input_size=X.shape[1])\n",
        "net.train()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# L2 regularization is included in Pytorch's optimizer implementation\n",
        "# as \"weigth_decay\" option\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# Train previously defined network for 20 epochs with SGD \n",
        "# and plot result for each epoch by uncommenting function below\n",
        "\n",
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "    # ============ YOUR CODE HERE ============\n",
        "\n",
        "\n",
        "    # For plotting and showing learning process at each epoch\n",
        "    # plot_decision_boundary(net, X, y, epoch, ((output>=0.5) == y).float().mean(), \n",
        "    #                       model_type='classic', tloc=TEXT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkFuifc5tknx"
      },
      "source": [
        "**[Question 1.1]: Analyze the results provided by previous plot. Looking at $p(\\mathbf{y}=1 | \\pmb{x}, \\pmb{w}_{\\textrm{MAP}})$, what can you say about points far from train distribution?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTozn8kQccU"
      },
      "source": [
        "### I.2 Laplace Approximation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BadWX2XfWEZe"
      },
      "source": [
        "We will use Laplace approximation to estimate the intractable posterior $p(\\pmb{w} | \\mathcal{D})$.\r\n",
        "\r\n",
        "Here, $p(\\pmb{w} | \\mathcal{D})$ is approximated with a normal distribution $\\mathcal{N}(\\pmb{w} ; \\pmb{\\mu}_{lap}, \\pmb{\\Sigma}_{lap}^2)$ where: \r\n",
        "\r\n",
        "- the mean of the normal distribution $\\pmb{\\mu}_{lap}$ corresponds to the mode of $p(\\pmb{w} | \\mathcal{D})$. In other words, it simply consists in taking the optimum weights of Maximum-A-Posteriori estimation : \r\n",
        "$$\\pmb{\\mu}_{lap} = \\pmb{w}_{\\textrm{MAP}} = \\arg \\min_{\\pmb{w}} -\\log p(\\pmb{w} | \\mathcal{D})$$. \r\n",
        "- the covariance matrix is obtained by computing the Hessian of the loss function $-\\log p(\\pmb{w} \\vert \\mathcal{D})$ at $\\pmb{w}=\\pmb{w}_{\\textrm{MAP}}$: \r\n",
        "$$(\\pmb{\\Sigma}^2_{lap})^{-1} = \\nabla\\nabla_{\\pmb{w}} [p(\\pmb{w} \\vert \\mathcal{D}) ]_{\\pmb{w}=\\pmb{w}_{\\textrm{MAP}}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LrZKf_dUs5q"
      },
      "source": [
        "#@title **[CODING TASK]** Extract μ_lap from previously trained model. \n",
        "# NB: Select only weights parameters (without bias)\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "w_map = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brSZS4nZx4_U"
      },
      "source": [
        "To compute the Hessian, we first compute the gradient at $\\pmb{w}_{\\textrm{MAP}}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEOmQfiHS6L-"
      },
      "source": [
        "# Computing first derivative w.r.t to model's weights\n",
        "optimizer.zero_grad()\n",
        "output = net(X).squeeze()\n",
        "loss = criterion(output, y) + net.fc.weight.norm()**2\n",
        "gradf_weight = grad(loss, net.fc.weight, create_graph=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADH0i667exQY"
      },
      "source": [
        "#@title **[CODING TASK]** Compute the Hessian from the previous derivative\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# Apply the same grad function on each scalar element of the gradient to get \n",
        "# each raw of the Hessian. Concatenate both and compute the covariance \n",
        "# by inverting the Hessian\n",
        "# NB: to avoid accumulated gradient when debugging and running the cell \n",
        "# multiple times, you should convert your grad results to numpy straight away\n",
        "hess_weights = None\n",
        "Sigma_laplace = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g7CKY0Myq1o"
      },
      "source": [
        "We now compute the posterior approximate $\\mathcal{N}(\\pmb{w} ; \\pmb{\\mu}_{lap}, \\pmb{\\Sigma}_{lap}^2)$ with the parameters found. \n",
        "\n",
        "Given this distribution, we can compute the posterior thanks to Monte-Carlo sampling and plot results for the last epoch corresponding to $\\pmb{w}_{\\textrm{MAP}}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xf9fF5OuB_K"
      },
      "source": [
        "# Defining posterior distribution\n",
        "laplace_posterior =  np.random.multivariate_normal(w_map.detach().numpy().reshape(2,), Sigma_laplace.detach().numpy(), NB_SAMPLES)\n",
        "\n",
        "# Plotting results\n",
        "fig, ax = plt.subplots(figsize=(7,7))\n",
        "plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), model_type='laplace', \n",
        "                       tloc=TEXT_LOCATION, nsamples=NB_SAMPLES, posterior=laplace_posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUPjVjot4IKE"
      },
      "source": [
        "**[Question 1.2]: Analyze the results provided by previous plot. Compared to previous MAP estimate, how does the predictive distribution behave?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgNFac420Nh_"
      },
      "source": [
        "### I.3 Variational Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuJcY6rMa7Ff"
      },
      "source": [
        "In this part, we will reimplement variational inference by hand with Pytorch tools. <br/><br/>\r\n",
        "\r\n",
        "**Optimization problem**  \r\n",
        "We define an approximating variational distribution $q_{\\pmb{\\theta}}(\\pmb{w})$ parametrized by $\\pmb{\\theta}$ and minimize its Kullback-Leibler (KL) divergence with the unknown true posterior $p(\\pmb{w} \\vert \\mathcal{D})$. This is equivalent to maximizing the **evidence lower bound (ELBO)** w.r.t to $q_{\\pmb{\\theta}}(\\pmb{w})$:\r\n",
        "\r\n",
        "$$ arg \\max_{\\pmb{\\theta}}~ \\mathbb{E}_{q_{\\pmb{\\theta}}(\\pmb{w})} \\big [\\underbrace{\\log p(\\mathcal{D} \\vert \\pmb{w})}_{likelihood} \\big ] + \\underbrace{\\textrm{KL}(q_{\\pmb{\\theta}}(\\pmb{w})\\vert\\vert p(\\pmb{w}))}_{regularization} $$\r\n",
        "where we have a likelihood term  and the KL divergence between the prior and the variational distribution.\r\n",
        "\r\n",
        "Let’s first rewrite the KL-divergence term:\r\n",
        "$$ \\textrm{KL}(q_{\\pmb{\\theta}}(\\pmb{w})\\vert\\vert p(\\pmb{w})) = \\int q_{\\pmb{\\theta}} \\log \\frac{q_{\\pmb{\\theta}}(\\pmb{w})}{p(\\pmb{w})}d\\pmb{w} = \\mathbb{E}_{q_{\\pmb{\\theta}}(\\pmb{w})}\\big [\\log q_{\\pmb{\\theta}}(\\pmb{w}) - \\log p(\\pmb{w})\\big ] $$\r\n",
        "As for the likelihood term, the KL-divergence can be written as an expectation over the approximate distribution $q_{\\pmb{\\theta}}(\\pmb{w})$.\r\n",
        "\r\n",
        "By assuming that samples are *i.i.id*, maximizing the ELBO is equivalent to minimizing the following loss:\r\n",
        "$$ \\mathcal{L}_{\\textrm{VI}}(\\pmb{\\theta}; \\mathcal{D}) = - \\sum_{n=1}^N \\mathbb{E}_{q_{\\pmb{\\theta}}(\\pmb{w})} \\Big [ \\log p(y_n \\vert \\pmb{x}_n, \\pmb{w}) + \\frac{1}{N} \\big ( \\log q_{\\pmb{\\theta}}(\\pmb{w}) - \\log p(\\pmb{w}) \\big ) \\Big ] $$\r\n",
        "The KL-divergence is not dependent on $\\mathcal{D}$, and can therefore be computed at the moment of sampling $\\pmb{w}$. <br/><br/>\r\n",
        "\r\n",
        "**Monte Carlo estimator**  \r\n",
        "Deriving those expectations can be some tedious mathematics, or maybe not even possible. Luckily we can get estimates of the mean by taking samples from $q_{\\pmb{\\theta}}(\\pmb{w})$ and average over those results.\r\n",
        "\r\n",
        "Even more simple, we can show that using only one sample is stil an unbiased gradient estimator. Hence, loss function simply boils down to minimzing at each step:\r\n",
        "\r\n",
        "$$ \\mathcal{L}_{\\textrm{VI}}(\\pmb{\\theta}; \\mathcal{D}) = - \\sum_{n=1}^N \\log p(y_n \\vert \\pmb{x}_n, \\pmb{w}_s) + \\big ( \\log q_{\\pmb{\\theta}}(\\pmb{w}_s) - \\log p(\\pmb{w}_s) \\big ) $$\r\n",
        "where $\\pmb{w}_s \\sim q_{\\pmb{\\theta}}$ is a sample from the variational distribution. <br/><br/>\r\n",
        "\r\n",
        "**Mean-field approximation**  \r\n",
        "For simplicity, we assume a factorisation over the weights on each layer:\r\n",
        "$$ q_{\\pmb{\\theta}}(\\pmb{w}) = \\prod_{l=1}^L q_{\\pmb{\\theta}}(\\pmb{W}_l) = \\prod_{l=1}^L \\mathcal{N}(\\pmb{W}_l; \\pmb{\\mu}_l, \\Sigma^2_l) $$\r\n",
        "As we chose a Gaussian prior $p(\\pmb{W}_l)$, we also defined here $q_{\\pmb{\\theta}}(\\pmb{W}_l)$ as a Gaussian distribution for its conjugate properties.\r\n",
        "<br/><br/>\r\n",
        "\r\n",
        "**Reparametrization trick**  \r\n",
        "If we start taking samples from $q_{\\pmb{\\theta}}$, we leave the deterministic world, and the gradient can not flow through the model anymore. We avoid this problem by reparameterizing the samples $\\pmb{W}_{l,s} \\sim \\mathcal{N}(\\pmb{\\mu}_{l}, \\pmb{\\Sigma}_{l}^2)$ from the distribution. \r\n",
        "Instead of sampling directly from the variational distribution, we sample from a unit gaussian and recreate samples from the variational distribution. Now the stochasticity of $\\varepsilon$ is external and will not prevent the flow of gradients.\r\n",
        "$$ \\pmb{W}_{l,s} = \\pmb{\\mu}_{l}+ \\pmb{\\Sigma}_{l}\\odot\\varepsilon $$\r\n",
        "where $\\varepsilon \\sim \\mathcal{N}(0,1)$.\r\n",
        "<br/><br/>  \r\n",
        "\r\n",
        "**Predictive distribution**  \r\n",
        "For a new sample $\\pmb{x^*}$, the predictive distribution can be approximated using **Monte Carlo sampling**:\r\n",
        "\\begin{equation}\r\n",
        "p(\\mathbf{y} =1|\\pmb{x}^*,\\mathcal{D}) \\approx \\int p(\\mathbf{y} = 1|x^*,w)q_\\theta^*(w) \\approx \\frac{1}{S} \\sum_{s=1}^S p(\\mathbf{y}=1|\\pmb{x}^*,\\pmb{w}_s)\r\n",
        "\\end{equation}\r\n",
        "where $\\pmb{w}_s \\sim q^*_{\\pmb{\\theta}}$ are samples from the optimum variational distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2eNammRvajL"
      },
      "source": [
        "#### Step 1: Implement a variational layer\r\n",
        "\r\n",
        "Let's first implement variational inference for a single layer. Remind that we defined our Logistic regression model as $f(x) = \\sigma(w^T x + b)$ where $\\sigma(t)= \\frac{1}{1+\\exp(t)}$ is the sigmoid function. As such, we need to place Gaussian distributions on parameters $w$ and $b$.  \r\n",
        "\r\n",
        "**Implementation constraint** Variance can not be negative. To avoid numerical issues, we will use $\\pmb{\\rho}$. Std can be retrieve with the following formula:\r\n",
        "$$ \\pmb{\\Sigma} = \\log(1 + e^{\\pmb{\\rho}}) $$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb6ofPETDZOb"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a variational layer from scratch\r\n",
        "\r\n",
        "class LinearVariational(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Mean field approximation of nn.Linear\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, input_size, output_size, parent):\r\n",
        "      super().__init__()\r\n",
        "      self.parent = parent\r\n",
        "      \r\n",
        "      if getattr(parent, 'accumulated_kl_div', None) is None:\r\n",
        "          parent.accumulated_kl_div = 0\r\n",
        "      \r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Initialize the variational parameters for weight and bias\r\n",
        "      # with nn.Parameter.\r\n",
        "      # Mean should be initialised to zeros and rho to ones\r\n",
        "      self.w_mu = None\r\n",
        "      self.w_rho = None\r\n",
        "      self.b_mu = None\r\n",
        "      self.b_rho = None\r\n",
        "        \r\n",
        "    def sampling(self, mu, rho):\r\n",
        "      \"Sample weights using the reparametrization trick\"\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Given parameter mu and rho, return sampling using \r\n",
        "      # the reparametrization trick.\r\n",
        "      # NB: you may look for torch.randn_like...\r\n",
        "      return None\r\n",
        "    \r\n",
        "    def kl_divergence(self, z, mu_theta, rho_theta, prior_sd=1):\r\n",
        "      \"Computing the KL-divergence term for these weight's parameters\"\r\n",
        "      log_prior = dist.Normal(0, prior_sd).log_prob(z) \r\n",
        "      log_p_q = dist.Normal(mu_theta, torch.log(1 + torch.exp(rho_theta))).log_prob(z)\r\n",
        "      return (log_p_q - log_prior).sum() / X.shape[0]\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      \"Usual forward function for pytorch layer\"\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Sample parameters w and b using self.sampling\r\n",
        "      # Then, perform a forward pass using those sampled parameters\r\n",
        "      out = None\r\n",
        "      \r\n",
        "      # Compute KL-div loss for training\r\n",
        "      self.parent.accumulated_kl_div += self.kl_divergence(w, self.w_mu, self.w_rho)\r\n",
        "      self.parent.accumulated_kl_div += self.kl_divergence(b, self.b_mu, self.b_rho)\r\n",
        "\r\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLazbzY0bIjw"
      },
      "source": [
        "#### Step 2: Variational Logistic Regression\r\n",
        "\r\n",
        "Now, let's use this `LinearVariational` layer in a Logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYGaD-CaD-Zt"
      },
      "source": [
        "class KL:\r\n",
        "    accumulated_kl_div = 0\r\n",
        "\r\n",
        "class VariationalLogisticRegression(nn.Module):\r\n",
        "    def __init__(self, input_size):\r\n",
        "        super().__init__()\r\n",
        "        self.kl_loss = KL\r\n",
        "        self.fc_var =  LinearVariational(input_size, 1, self.kl_loss)\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def accumulated_kl_div(self):\r\n",
        "        return self.kl_loss.accumulated_kl_div\r\n",
        "    \r\n",
        "    def reset_kl_div(self):\r\n",
        "        self.kl_loss.accumulated_kl_div = 0\r\n",
        "            \r\n",
        "    def forward(self, x):\r\n",
        "        out = self.fc_var(x)\r\n",
        "        return torch.sigmoid(out)\r\n",
        "\r\n",
        "\r\n",
        "def elbo(input, target, model):\r\n",
        "  negative_log_likelihood = -dist.Binomial(logits=input).log_prob(target).sum()\r\n",
        "  return negative_log_likelihood + model.accumulated_kl_div"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlRYhjTXxbvy"
      },
      "source": [
        "We can now train our variational model as any other network in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpq1SuhvD-0Y"
      },
      "source": [
        "var_net = VariationalLogisticRegression(input_size=X.shape[1])\r\n",
        "var_net.train()\r\n",
        "optimizer = torch.optim.Adam(var_net.parameters(), lr=0.1,  weight_decay=WEIGHT_DECAY)\r\n",
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "\r\n",
        "for epoch in range(30):  # loop over the dataset multiple times\r\n",
        "    # zero the parameter gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    var_net.reset_kl_div()\r\n",
        "\r\n",
        "    # forward + backward + optimize\r\n",
        "    output = var_net(X).squeeze()\r\n",
        "    loss = elbo(output, y, var_net)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # Computing prediction for visualization purpose\r\n",
        "    preds = torch.zeros(NB_SAMPLES, X.shape[0], 1)\r\n",
        "    for i in range(NB_SAMPLES):\r\n",
        "        preds[i] = var_net(X)\r\n",
        "    pred = preds.mean(0).squeeze()\r\n",
        "    accuracy = ((pred>=0.5) == y).float().mean()\r\n",
        "\r\n",
        "    # For plotting and showing learning process at each epoch\r\n",
        "    plot_decision_boundary(var_net, X, y, epoch, accuracy, model_type='vi', tloc=TEXT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRbXTWZU48w6"
      },
      "source": [
        "**[Question 1.3]: Analyze the results provided by previous plot. Compared to previous MAP estimate, how does the predictive distribution behave?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZV6pC6v7Bqh"
      },
      "source": [
        "## Part II: Bayesian Neural Networks\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqscdtHyT97"
      },
      "source": [
        "Moving on to a non-linear dataset, we will leverage our variational implementation to a Multi-Layer Perceptron (MLP). Finally, we will also review one last approximate inference method which has the particularity to be very easy to implement: Monte-Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g0QmYJ7WJ8p"
      },
      "source": [
        "### II.0 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVUva--mk2jk",
        "cellView": "form"
      },
      "source": [
        "#@title Hyperparameters for model and approximate inference { form-width: \"30%\" }\r\n",
        "\r\n",
        "NOISE_MOON = 0.05 #@param\r\n",
        "WEIGHT_DECAY = 5e-2 #@param\r\n",
        "NB_SAMPLES = 100 #@param\r\n",
        "TEXT_LOCATION = (-1.5, -1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjIhWLPrkVdp"
      },
      "source": [
        "# Load two moons dataset\r\n",
        "X, y = make_moons (n_samples=1000, noise=NOISE_MOON)\r\n",
        "X, y = torch.from_numpy(X), torch.from_numpy(y)\r\n",
        "X, y = X.type(torch.float), y.type(torch.float)\r\n",
        "torch_train_dataset = data.TensorDataset(X,y) # create your datset\r\n",
        "train_dataloader = data.DataLoader(torch_train_dataset, batch_size=len(torch_train_dataset))\r\n",
        "N_DIM = X.shape[1]\r\n",
        "\r\n",
        "# Visualize dataset\r\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='Paired_r', edgecolors='k')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0rMZBLxbHY9"
      },
      "source": [
        "### II.1 Variational Inference with Bayesian Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV8BP1t1yuu1"
      },
      "source": [
        "Such as for Logistic Regression, we will use `LinearVariational` layer to define a MLP with 1 hidden layer.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCOP9aDQo7zh"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a Variational MLP\r\n",
        "\r\n",
        "class VariationalMLP(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "      super().__init__()\r\n",
        "      self.kl_loss = KL\r\n",
        "\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Define a variational MLP with 1 hidden layer and ReLU activation\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def accumulated_kl_div(self):\r\n",
        "      return self.kl_loss.accumulated_kl_div\r\n",
        "    \r\n",
        "    def reset_kl_div(self):\r\n",
        "      self.kl_loss.accumulated_kl_div = 0\r\n",
        "            \r\n",
        "    def forward(self, x):\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Don't forget to apply the sigmoid function when returning the output\r\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7NXnFs0AmB"
      },
      "source": [
        "We can now train our variational model as any other network in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar5ZX1SfpdgV"
      },
      "source": [
        "var_net = VariationalMLP(input_size=X.shape[1], hidden_size=50)\r\n",
        "var_net.train()\r\n",
        "optimizer = torch.optim.Adam(var_net.parameters(), lr=0.1, weight_decay=WEIGHT_DECAY)\r\n",
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "\r\n",
        "for epoch in range(1000):  # loop over the dataset multiple times\r\n",
        "    # zero the parameter gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    var_net.reset_kl_div()\r\n",
        "\r\n",
        "    # forward + backward + optimize\r\n",
        "    output = var_net(X).squeeze()\r\n",
        "    loss = elbo(output, y, var_net)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # For plotting and showing learning process at each epoch\r\n",
        "    if (epoch+1)%50==0:\r\n",
        "      # Computing prediction for visualization purpose\r\n",
        "      preds = torch.zeros(NB_SAMPLES, X.shape[0], 1)\r\n",
        "      for i in range(NB_SAMPLES):\r\n",
        "          preds[i] = var_net(X)\r\n",
        "      pred = preds.mean(0).squeeze()\r\n",
        "      accuracy = ((pred>=0.5) == y).float().mean()\r\n",
        "\r\n",
        "      plot_decision_boundary(var_net, X, y, epoch, accuracy, model_type='vi', tloc=TEXT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_scDA7bC1c"
      },
      "source": [
        "### II.2 Monte Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8J9UNT41TJ2"
      },
      "source": [
        "Training a neural network with randomly dropping some activations, such as with dropout layers, can actually be seen as an **approximate variational inference method**!\r\n",
        "\r\n",
        "[Gal and Ghahramani, 2016] showed this can be fullfilled for:\r\n",
        "- $p(\\pmb{w}) = \\prod_l p(\\pmb{W}_l) = \\prod_l \\mathcal{MN}(\\pmb{W}_l; 0, I/ l_i^2, I)$ $\\Rightarrow$ Multivariate Gaussian distribution factorized over layers\r\n",
        "- $q(\\pmb{w}) = \\prod_l q(\\pmb{W}_l) = \\prod_l \\textrm{diag}(\\varepsilon_l)\\odot\\pmb{M}_l $ with $\\varepsilon_l \\sim \\textrm{Ber}(1-p_l)$.\r\n",
        "\r\n",
        "We will now implement a MLP with dropout layers and perform Monte-Carlo sampling to obtain the predictive distribution $p(\\mathbf{y} \\vert \\pmb{x}^*, \\pmb{w})$ for a new sample $\\pmb{x}^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxGmkjd0kVgZ"
      },
      "source": [
        "#@title **[CODING TASK]** Implement a MLP with dropout (p=0.2)\r\n",
        "# Code MLP with 1 hidden layer and a dropout layer. Be careful, the dropout \r\n",
        "# layer should be also activated during test time.\r\n",
        "# (Hint: we may want to look out at F.dropout())\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "    \"\"\" Pytorch MLP for binary classification model with an added dropout layer\"\"\"\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      # ============ YOUR CODE HERE ============\r\n",
        "      # Don't forget to apply the sigmoid function when returning the output\r\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMlasuRN08Yk"
      },
      "source": [
        "We train our model as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqSZJDWIlf5e"
      },
      "source": [
        "net = MLP(input_size=X.shape[1], hidden_size=50)\r\n",
        "net.train()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\r\n",
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "\r\n",
        "for epoch in range(500):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    # zero the parameter gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # forward + backward + optimize\r\n",
        "    output = net(X).squeeze()\r\n",
        "    loss = criterion(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # For plotting and showing learning process at each epoch, uncomment and indent line below\r\n",
        "    if (epoch+1)%50==0:\r\n",
        "      plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='classic')\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_RyglMRa4d5"
      },
      "source": [
        "Now let's look at the results given by MC Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRh6BQDcazFx"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7))\r\n",
        "plot_decision_boundary(net, X, y, epoch, ((output.squeeze()>=0.5) == y).float().mean(), tloc=TEXT_LOCATION, model_type='mcdropout')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05OBuwko5EvS"
      },
      "source": [
        "**[Question 2.1]: Again, analyze the results showed on plot. What is the benefit of MC Dropout variational inference over Bayesian Logistic Regression with variational inference?**"
      ]
    }
  ]
}