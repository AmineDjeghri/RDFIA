{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbzBJ1m9FBBb"
   },
   "source": [
    "# Attention : \n",
    "# Faire \"File -> Save a copy in Drive\" avant de commencer à modifier le notebook, sinon vos modifications ne seront pas sauvegardées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "NfnKy8NB8J5e",
    "outputId": "5d383487-d153-4ffd-b944-312b7a8f3813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-30 17:20:07--  http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n",
      "Resolving webia.lip6.fr (webia.lip6.fr)... 132.227.201.33\n",
      "Connecting to webia.lip6.fr (webia.lip6.fr)|132.227.201.33|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13421167 (13M) [application/zip]\n",
      "Saving to: ‘TP3-4.zip’\n",
      "\n",
      "TP3-4.zip           100%[===================>]  12.80M  11.2MB/s    in 1.1s    \n",
      "\n",
      "2020-09-30 17:20:08 (11.2 MB/s) - ‘TP3-4.zip’ saved [13421167/13421167]\n",
      "\n",
      "Archive:  TP3-4.zip\n",
      "  inflating: tme5.py                 \n",
      "  inflating: mnist.mat               \n",
      "  inflating: circles.py              \n",
      "  inflating: circles.mat             \n"
     ]
    }
   ],
   "source": [
    "!wget http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n",
    "!unzip -j TP3-4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20219,
     "status": "ok",
     "timestamp": 1607744386447,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "54lNh5hoRv3T",
    "outputId": "bc12686b-6561-420e-cd1c-52560df09652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      " circles.mat\n",
      " circles.py\n",
      "'cours 2 a.pdf'\n",
      "'cours 2 b.pdf'\n",
      "'cours 2 c.pdf'\n",
      " DJEGHRI_MAMOU_TP_3_4_Introduction_aux_réseaux_de_neurones_Sorbonne.ipynb\n",
      " mnist.mat\n",
      "'Nouveau Document Microsoft Word.docx'\n",
      "'Rapport 3-4 DJEGHRI_MAMOU .pdf'\n",
      " tme5.py\n",
      " TP_3_4_Introduction_aux_réseaux_de_neurones_Sorbonne.ipynb\n",
      " tp3-4.pdf\n",
      "'tp3-4 solutions gradient.pdf'\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/git/RDFIA/TME 3-4\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1607744393398,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "2vQ_LLdx8J5b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%run tme5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48x_ha7f8J5i"
   },
   "source": [
    "# Partie 1 : Forward et Backward manuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1607744397838,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "GtizX1JV8J5n"
   },
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    nx, nh, ny: integers\n",
    "    out params: dictionnary\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    \n",
    "    # remplir avec les paramètres Wh, Wy, bh, by\n",
    "\n",
    "    params[\"Wh\"] = torch.normal(mean=0, std=0.3, size=(nh,nx))\n",
    "                # or torch.randn(nh, nx).normal_(mean=0,std=0.3)\n",
    "    params[\"Wy\"] = torch.normal(mean=0, std=0.3, size=(ny,nh))\n",
    "    params[\"bh\"] = torch.normal(mean=0, std=0.3, size=(nh,))\n",
    "    params[\"by\"] = torch.normal(mean=0, std=0.3, size=(ny,))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1607744399476,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "jk-N_Ny67yo-"
   },
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    \"\"\"\n",
    "    params: dictionnary\n",
    "    X: (n_batch, dimension)\n",
    "    \"\"\"\n",
    "    bsize = X.size(0)\n",
    "    nh = params['Wh'].size(0)\n",
    "    ny = params['Wy'].size(0)\n",
    "    outputs = {}\n",
    "\n",
    "\n",
    "    # remplir avec les paramètres X, htilde, h, ytilde, yhat\n",
    "    \n",
    "    outputs[\"X\"] = X\n",
    "    outputs[\"htilde\"] = torch.mm(X,params[\"Wh\"].T) + params[\"bh\"]\n",
    "    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n",
    "    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"],params[\"Wy\"].T) + params[\"by\"]\n",
    "    outputs[\"yhat\"] = torch.exp(outputs[\"ytilde\"]) / torch.sum(torch.exp(outputs[\"ytilde\"]), dim=1, keepdim=True)\n",
    "\n",
    "    return outputs['yhat'], outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1607744400398,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "-uB0A2b28NZK"
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(Yhat, Y):\n",
    "\n",
    "\n",
    "    L = 0\n",
    "    acc = 0\n",
    "\n",
    "    L = -torch.mean(torch.sum(Y*torch.log(Yhat)))\n",
    "\n",
    "    _, indsY = torch.max(Y, 1)\n",
    "    _, indsYhat = torch.max(Yhat, 1)\n",
    "    acc = (indsY == indsYhat).float().mean()*100\n",
    "\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1607744401237,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "WWJjdiFe8qi5"
   },
   "outputs": [],
   "source": [
    "def backward(params, outputs, Y):\n",
    "    bsize = Y.shape[0]\n",
    "    grads = {}\n",
    "\n",
    "    # remplir avec les paramètres Wy, Wh, by, bh\n",
    "\n",
    "    # dérivé de L par rapport à ytilde\n",
    "    grad_L_ytilde = outputs['yhat'] - Y \n",
    "\n",
    "    # dérivé de ytilde par rapport à Wh\n",
    "    grad_ytilde_Wh = outputs['h']\n",
    "    # dérivé de ytilde par rapport à Bh\n",
    "    grad_ytilde_bh = 1\n",
    "\n",
    "\n",
    "    grads[\"Wy\"] = torch.mm(grad_L_ytilde.T, grad_ytilde_Wh)   # dérivé de L par rapport à Wy\n",
    "    grads[\"by\"] = torch.sum(grad_L_ytilde, dim=0)   # dérivé de L par rapport à by\n",
    "\n",
    "    # dérivé de y_tilde par rapport à h\n",
    "    grad_ytilde_h = params['Wy']\n",
    "    # dérivé de L par rapport à htilde\n",
    "    grad_h_htilde = torch.mm(grad_L_ytilde,grad_ytilde_h) * (1-torch.pow(grad_ytilde_Wh,2))\n",
    "\n",
    "    grads[\"Wh\"] = torch.mm(grad_h_htilde, outputs['X'])  # dérivé de L par rapport à Wh\n",
    "    \n",
    "    grads[\"bh\"] = torch.sum(grad_h_htilde, dim=0)  # dérivé de L par rapport à bh\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1607744402959,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "nAnsISsW9CnH"
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, eta):\n",
    "    params[\"Wh\"] = params[\"Wh\"] - eta * grads['Wh']  \n",
    "    params[\"Wy\"] = params[\"Wy\"] - eta * grads['Wy']  \n",
    "    params[\"bh\"] = params[\"bh\"] - eta * grads['bh']  \n",
    "    params[\"by\"] = params[\"by\"] - eta * grads['by']    \n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hifuW5UFA3DZ"
   },
   "source": [
    "## Algorithme global d'apprentissage (manuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1VxoqNlAE9d58_p9polKKuIYDE-scIj83"
    },
    "executionInfo": {
     "elapsed": 28319,
     "status": "ok",
     "timestamp": 1607744432901,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "4RSw6bd0-qUe",
    "outputId": "50ba1ee4-dff1-43e9-cd5c-191b23a34323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "params = init_params(nx, nh, ny)\n",
    "\n",
    "curves = [[],[], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(150):\n",
    "\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "\n",
    "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch, :]\n",
    "\n",
    "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
    "        # en utilisant les fonctions forward, loss_accuracy, backward, sgd \n",
    "\n",
    "        Yhat, outputs = forward(params, X)\n",
    "        loss, acc = loss_accuracy(Yhat, Y)\n",
    "        grads = backward(params, outputs, Y) \n",
    "        params = sgd(params, grads, eta)\n",
    "\n",
    "    Yhat_train, _ = forward(params, data.Xtrain)\n",
    "    Yhat_test, _ = forward(params, data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
    "    Ygrid, _ = forward(params, data.Xgrid)  \n",
    "\n",
    "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
    "    print(title)\n",
    "    data.plot_data_with_grid(Ygrid, title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain)\n",
    "    curves[3].append(Ltest)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrHHH5PL8J54"
   },
   "source": [
    "# Partie 2 : Simplification du backward avec `torch.autograd`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1607744440726,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "7G4q5zP0CEvB"
   },
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    nx, nh, ny: integers\n",
    "    out params: dictionnary\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    \n",
    "\n",
    "    # remplir avec les paramètres Wh, Wy, bh, by\n",
    "    # activer l'autograd sur les poids du réseau\n",
    "    \n",
    "    params[\"Wh\"] = torch.normal(mean=0, std=0.3, size=(nh, nx), requires_grad=True)\n",
    "    params[\"Wy\"] = torch.normal(mean=0, std=0.3, size=(ny, nh), requires_grad=True)\n",
    "    params[\"bh\"] = torch.normal(mean=0, std=0.3, size=(nh,), requires_grad=True)\n",
    "    params[\"by\"] = torch.normal(mean=0, std=0.3, size=(ny,), requires_grad=True)\n",
    "    \n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL0tSjpKCyVB"
   },
   "source": [
    "La fonction `forward` est inchangée par rapport à la partie précédente. \n",
    "\n",
    "La fonction `backward` n'est plus utilisée grâce à l'autograd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1607744441771,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "hA4ycHlfBzCK"
   },
   "outputs": [],
   "source": [
    "def sgd(params, eta):\n",
    "\n",
    "    # mettre à jour le contenu de params\n",
    "    # attention à bien utiliser torch.no_grad()\n",
    "    # et à remettre les accumulateurs de gradients à zéro\n",
    "    with torch.no_grad():\n",
    "        for param in params.values():\n",
    "          param-= eta*param.grad\n",
    "          param.grad.zero_()\n",
    "\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjgcmgQpDfOb"
   },
   "source": [
    "## Algorithme global d'apprentissage (avec autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "12Zu9L7GTaEqF_UMqXR_vuUTVTUX8-H6w"
    },
    "executionInfo": {
     "elapsed": 28531,
     "status": "ok",
     "timestamp": 1607744472260,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "8p5oR3EqDea-",
    "outputId": "29223f9f-d8aa-4722-d91e-125c885a9ff7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "params = init_params(nx, nh, ny)\n",
    "\n",
    "curves = [[],[], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(150):\n",
    "\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "\n",
    "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch, :]\n",
    "  \n",
    "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
    "        # en utilisant les fonctions forward, loss_accuracy, sgd \n",
    "        # calculer les gradients avec la fonction backward de l'autograd \n",
    "\n",
    "        Yhat, outputs = forward(params, X)\n",
    "        loss, acc = loss_accuracy(Yhat, Y)\n",
    "        loss.backward()\n",
    "        params = sgd(params, eta)\n",
    "\n",
    "\n",
    "\n",
    "    Yhat_train, _ = forward(params, data.Xtrain)\n",
    "    Yhat_test, _ = forward(params, data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
    "    Ygrid, _ = forward(params, data.Xgrid)  \n",
    "\n",
    "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
    "    print(title)\n",
    "    # detach() est utilisé pour détacher les predictions du graphes de calcul autograd\n",
    "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain)\n",
    "    curves[3].append(Ltest)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FV1iss68J6H"
   },
   "source": [
    "# Partie 3 : Simplification du forward avec `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6T5Uq7JEl47"
   },
   "source": [
    "`init_params` et `forward` sont supprimés et remplacés par une fonction `init_model` qui déclare l'architecture du modèle et la loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 25112,
     "status": "ok",
     "timestamp": 1607744472264,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "5-h4r-FH8J6I"
   },
   "outputs": [],
   "source": [
    "def init_model(nx, nh, ny):\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nx, nh),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(nh, ny),\n",
    "    )\n",
    "    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 24347,
     "status": "ok",
     "timestamp": 1607744472266,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "geE_TI96FXnl"
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(loss, Yhat, Y):\n",
    "\n",
    "    # faire appel à la fonction de loss\n",
    "\n",
    "    L = loss(Yhat, Y)\n",
    "    _, indsYhat = torch.max(Yhat, 1)\n",
    "    acc = (Y == indsYhat).float().mean()*100\n",
    "\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 24139,
     "status": "ok",
     "timestamp": 1607744472268,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "e93bvFiYGKnA"
   },
   "outputs": [],
   "source": [
    "def sgd(model, eta):\n",
    "\n",
    "    # mettre à jour les paramètres de model\n",
    "    # attention à bien utiliser torch.no_grad()\n",
    "    # et à remettre les accumulateurs de gradients à zéro\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= eta * param.grad\n",
    "        model.zero_grad()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOxBMmD4Gxtp"
   },
   "source": [
    "## Algorithme global d'apprentissage (avec autograd et les couches `torch.nn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1eLQ7hgjn_QwTR_eGB9oviThp5Cs69j6f"
    },
    "executionInfo": {
     "elapsed": 51112,
     "status": "ok",
     "timestamp": 1607744500388,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "4hMBmCNvHCLn",
    "outputId": "fe26d81f-a06d-4176-e7e6-87c908c38926"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "model, loss = init_model(nx, nh, ny)\n",
    "\n",
    "curves = [[],[], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(150):\n",
    "\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "    Ytrain = torch.argmax(Ytrain, dim=1)\n",
    "\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "\n",
    "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch]\n",
    "  \n",
    "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
    "        # en utilisant les fonctions loss_accuracy, sgd\n",
    "        # effectuer le forward en faisant un appel au modèle \n",
    "        # calculer les gradients avec la fonction backward de l'autograd \n",
    "\n",
    "        Yhat = model(X)\n",
    "        l, acc = loss_accuracy(loss, Yhat, Y)\n",
    "        l.backward()\n",
    "        sgd(model, eta)\n",
    "\n",
    "\n",
    "\n",
    "    Yhat_train = model(data.Xtrain)\n",
    "    Yhat_test = model(data.Xtest)\n",
    "\n",
    "    Ytrain = torch.argmax(data.Ytrain, dim=1)\n",
    "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n",
    "    Ytest = torch.argmax(data.Ytest, dim=1)\n",
    "    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n",
    "    Ygrid = model(data.Xgrid)  \n",
    "\n",
    "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
    "    print(title) \n",
    "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain)\n",
    "    curves[3].append(Ltest)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoFSrQNsJCnz"
   },
   "source": [
    "# Partie 4 : Simplification de SGD avec `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 49656,
     "status": "ok",
     "timestamp": 1607744500390,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "S8WtN9loJPqP"
   },
   "outputs": [],
   "source": [
    "def init_model(nx, nh, ny, eta):\n",
    "  \n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nx, nh),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(nh, ny),\n",
    "    )\n",
    "    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "    return model, loss, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY-0rRzPJYDd"
   },
   "source": [
    "La fonction `sgd` est supprimée. À la place, on fera un appel à `optim.zero_grad()` avant de faire le backward et à `optim.step()` après le backward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q82hCupvJxvV"
   },
   "source": [
    "## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LzOwhsEA6kax05eXN4S91xyAA1J8nr2v"
    },
    "executionInfo": {
     "elapsed": 74731,
     "status": "ok",
     "timestamp": 1607744528593,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "V9h9nINKJ1LU",
    "outputId": "deec5ba7-9003-49e2-e0de-b1e4cb66324a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "model, loss, optim = init_model(nx, nh, ny, eta)\n",
    "\n",
    "curves = [[],[], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(150):\n",
    "\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "    Ytrain = torch.argmax(Ytrain, dim=1)\n",
    "\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "\n",
    "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch]\n",
    "\n",
    "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
    "        # en utilisant la fonction loss_accuracy\n",
    "        # effectuer le forward en faisant un appel au modèle \n",
    "        # calculer les gradients avec la fonction backward de l'autograd \n",
    "        # Puis une \"step\" d'optimisation\n",
    "\n",
    "        Yhat = model(X)\n",
    "        L= loss(Yhat, Y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        L.backward()\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "\n",
    "    Yhat_train = model(data.Xtrain)\n",
    "    Yhat_test = model(data.Xtest)\n",
    "    Ytrain = torch.argmax(data.Ytrain, dim=1)\n",
    "    Ytest = torch.argmax(data.Ytest, dim=1)\n",
    "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n",
    "    Ygrid = model(data.Xgrid)  \n",
    "\n",
    "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
    "    print(title) \n",
    "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain)\n",
    "    curves[3].append(Ltest)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts1s4JuOSaZ3"
   },
   "source": [
    "# Partie 5 : MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jly9C4FCSzLP"
   },
   "source": [
    "Reprendre le code d'entraînement précédent en l'appliquant au dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321905,
     "status": "ok",
     "timestamp": 1607744778554,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "osrFoEr_Syi7",
    "outputId": "ef0b1524-b1ec-4bd9-a729-217ca7e92170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Acc train 85.6% (0.52), acc test 87.0% (0.49)\n",
      "Iter 1: Acc train 87.3% (0.46), acc test 87.6% (0.45)\n",
      "Iter 2: Acc train 87.8% (0.41), acc test 88.2% (0.40)\n",
      "Iter 3: Acc train 88.5% (0.39), acc test 89.2% (0.37)\n",
      "Iter 4: Acc train 88.0% (0.40), acc test 88.2% (0.39)\n",
      "Iter 5: Acc train 88.8% (0.38), acc test 88.8% (0.38)\n",
      "Iter 6: Acc train 88.9% (0.38), acc test 89.3% (0.36)\n",
      "Iter 7: Acc train 88.1% (0.39), acc test 88.4% (0.38)\n",
      "Iter 8: Acc train 90.2% (0.33), acc test 90.1% (0.33)\n",
      "Iter 9: Acc train 89.6% (0.35), acc test 89.2% (0.36)\n",
      "Iter 10: Acc train 89.7% (0.36), acc test 89.7% (0.35)\n",
      "Iter 11: Acc train 90.3% (0.33), acc test 90.7% (0.31)\n",
      "Iter 12: Acc train 88.9% (0.36), acc test 89.4% (0.35)\n",
      "Iter 13: Acc train 90.9% (0.30), acc test 90.9% (0.30)\n",
      "Iter 14: Acc train 88.8% (0.36), acc test 88.9% (0.36)\n",
      "Iter 15: Acc train 88.4% (0.38), acc test 88.4% (0.38)\n",
      "Iter 16: Acc train 90.0% (0.33), acc test 90.2% (0.33)\n",
      "Iter 17: Acc train 90.8% (0.31), acc test 90.8% (0.31)\n",
      "Iter 18: Acc train 90.0% (0.33), acc test 89.8% (0.33)\n",
      "Iter 19: Acc train 90.7% (0.31), acc test 90.6% (0.31)\n",
      "Iter 20: Acc train 90.1% (0.32), acc test 89.6% (0.33)\n",
      "Iter 21: Acc train 90.4% (0.31), acc test 90.5% (0.31)\n",
      "Iter 22: Acc train 90.4% (0.32), acc test 90.5% (0.32)\n",
      "Iter 23: Acc train 90.3% (0.32), acc test 90.4% (0.31)\n",
      "Iter 24: Acc train 90.6% (0.31), acc test 90.7% (0.30)\n",
      "Iter 25: Acc train 90.7% (0.31), acc test 90.3% (0.31)\n",
      "Iter 26: Acc train 90.8% (0.31), acc test 90.7% (0.31)\n",
      "Iter 27: Acc train 91.0% (0.30), acc test 91.4% (0.29)\n",
      "Iter 28: Acc train 90.0% (0.33), acc test 90.5% (0.31)\n",
      "Iter 29: Acc train 90.7% (0.30), acc test 91.3% (0.29)\n",
      "Iter 30: Acc train 90.5% (0.32), acc test 90.4% (0.31)\n",
      "Iter 31: Acc train 90.9% (0.30), acc test 91.1% (0.29)\n",
      "Iter 32: Acc train 89.9% (0.32), acc test 90.4% (0.32)\n",
      "Iter 33: Acc train 90.5% (0.31), acc test 90.8% (0.30)\n",
      "Iter 34: Acc train 91.1% (0.29), acc test 91.3% (0.29)\n",
      "Iter 35: Acc train 90.9% (0.29), acc test 90.9% (0.29)\n",
      "Iter 36: Acc train 91.1% (0.29), acc test 91.0% (0.29)\n",
      "Iter 37: Acc train 91.5% (0.28), acc test 91.8% (0.28)\n",
      "Iter 38: Acc train 91.7% (0.28), acc test 92.2% (0.27)\n",
      "Iter 39: Acc train 91.4% (0.28), acc test 91.5% (0.28)\n",
      "Iter 40: Acc train 90.5% (0.31), acc test 90.8% (0.30)\n",
      "Iter 41: Acc train 90.6% (0.31), acc test 90.9% (0.30)\n",
      "Iter 42: Acc train 91.8% (0.27), acc test 92.0% (0.27)\n",
      "Iter 43: Acc train 91.7% (0.27), acc test 91.9% (0.28)\n",
      "Iter 44: Acc train 91.3% (0.29), acc test 91.3% (0.28)\n",
      "Iter 45: Acc train 90.9% (0.30), acc test 91.5% (0.29)\n",
      "Iter 46: Acc train 91.5% (0.28), acc test 91.7% (0.28)\n",
      "Iter 47: Acc train 91.8% (0.27), acc test 91.8% (0.26)\n",
      "Iter 48: Acc train 91.7% (0.27), acc test 92.1% (0.26)\n",
      "Iter 49: Acc train 91.5% (0.28), acc test 91.8% (0.28)\n",
      "Iter 50: Acc train 91.4% (0.28), acc test 91.8% (0.28)\n",
      "Iter 51: Acc train 91.3% (0.28), acc test 91.4% (0.28)\n",
      "Iter 52: Acc train 92.1% (0.26), acc test 92.6% (0.26)\n",
      "Iter 53: Acc train 91.5% (0.27), acc test 91.5% (0.27)\n",
      "Iter 54: Acc train 92.1% (0.26), acc test 91.8% (0.27)\n",
      "Iter 55: Acc train 91.7% (0.27), acc test 91.7% (0.27)\n",
      "Iter 56: Acc train 92.0% (0.26), acc test 91.9% (0.26)\n",
      "Iter 57: Acc train 91.6% (0.27), acc test 91.7% (0.27)\n",
      "Iter 58: Acc train 91.8% (0.27), acc test 91.8% (0.27)\n",
      "Iter 59: Acc train 92.0% (0.26), acc test 92.1% (0.26)\n",
      "Iter 60: Acc train 92.3% (0.26), acc test 92.3% (0.26)\n",
      "Iter 61: Acc train 92.1% (0.25), acc test 92.3% (0.26)\n",
      "Iter 62: Acc train 91.5% (0.27), acc test 91.6% (0.28)\n",
      "Iter 63: Acc train 92.0% (0.26), acc test 92.1% (0.26)\n",
      "Iter 64: Acc train 92.2% (0.26), acc test 92.5% (0.25)\n",
      "Iter 65: Acc train 92.2% (0.26), acc test 92.4% (0.26)\n",
      "Iter 66: Acc train 92.6% (0.24), acc test 92.7% (0.25)\n",
      "Iter 67: Acc train 92.1% (0.26), acc test 92.4% (0.26)\n",
      "Iter 68: Acc train 92.1% (0.25), acc test 92.3% (0.26)\n",
      "Iter 69: Acc train 92.0% (0.26), acc test 92.0% (0.27)\n",
      "Iter 70: Acc train 92.1% (0.26), acc test 92.2% (0.26)\n",
      "Iter 71: Acc train 91.7% (0.27), acc test 91.8% (0.27)\n",
      "Iter 72: Acc train 92.2% (0.26), acc test 92.2% (0.26)\n",
      "Iter 73: Acc train 92.6% (0.24), acc test 92.6% (0.25)\n",
      "Iter 74: Acc train 92.5% (0.25), acc test 92.4% (0.25)\n",
      "Iter 75: Acc train 92.6% (0.24), acc test 92.7% (0.24)\n",
      "Iter 76: Acc train 92.3% (0.25), acc test 92.0% (0.26)\n",
      "Iter 77: Acc train 92.8% (0.24), acc test 93.0% (0.24)\n",
      "Iter 78: Acc train 92.8% (0.24), acc test 92.7% (0.25)\n",
      "Iter 79: Acc train 93.2% (0.22), acc test 93.3% (0.23)\n",
      "Iter 80: Acc train 93.1% (0.23), acc test 93.1% (0.23)\n",
      "Iter 81: Acc train 92.5% (0.24), acc test 92.4% (0.26)\n",
      "Iter 82: Acc train 93.0% (0.23), acc test 92.8% (0.25)\n",
      "Iter 83: Acc train 92.4% (0.25), acc test 92.4% (0.25)\n",
      "Iter 84: Acc train 92.7% (0.23), acc test 92.9% (0.24)\n",
      "Iter 85: Acc train 93.0% (0.23), acc test 92.6% (0.24)\n",
      "Iter 86: Acc train 92.7% (0.24), acc test 92.7% (0.24)\n",
      "Iter 87: Acc train 91.9% (0.26), acc test 91.5% (0.27)\n",
      "Iter 88: Acc train 92.1% (0.26), acc test 92.1% (0.26)\n",
      "Iter 89: Acc train 92.6% (0.24), acc test 92.5% (0.25)\n",
      "Iter 90: Acc train 92.5% (0.24), acc test 92.7% (0.25)\n",
      "Iter 91: Acc train 92.2% (0.26), acc test 92.0% (0.26)\n",
      "Iter 92: Acc train 92.0% (0.26), acc test 92.0% (0.26)\n",
      "Iter 93: Acc train 92.3% (0.25), acc test 92.4% (0.25)\n",
      "Iter 94: Acc train 92.3% (0.25), acc test 92.3% (0.25)\n",
      "Iter 95: Acc train 92.2% (0.25), acc test 92.5% (0.25)\n",
      "Iter 96: Acc train 93.0% (0.23), acc test 93.1% (0.24)\n",
      "Iter 97: Acc train 93.0% (0.23), acc test 93.3% (0.23)\n",
      "Iter 98: Acc train 93.2% (0.23), acc test 93.1% (0.23)\n",
      "Iter 99: Acc train 93.1% (0.23), acc test 93.2% (0.23)\n",
      "Iter 100: Acc train 93.3% (0.22), acc test 93.0% (0.23)\n",
      "Iter 101: Acc train 93.1% (0.23), acc test 92.8% (0.24)\n",
      "Iter 102: Acc train 92.9% (0.23), acc test 92.5% (0.24)\n",
      "Iter 103: Acc train 93.4% (0.22), acc test 93.4% (0.23)\n",
      "Iter 104: Acc train 93.1% (0.23), acc test 93.2% (0.23)\n",
      "Iter 105: Acc train 93.4% (0.22), acc test 92.8% (0.23)\n",
      "Iter 106: Acc train 92.0% (0.26), acc test 91.9% (0.26)\n",
      "Iter 107: Acc train 92.8% (0.23), acc test 93.0% (0.23)\n",
      "Iter 108: Acc train 92.9% (0.23), acc test 92.9% (0.23)\n",
      "Iter 109: Acc train 93.2% (0.22), acc test 93.0% (0.22)\n",
      "Iter 110: Acc train 93.1% (0.23), acc test 93.1% (0.23)\n",
      "Iter 111: Acc train 92.7% (0.24), acc test 93.1% (0.24)\n",
      "Iter 112: Acc train 92.5% (0.25), acc test 92.2% (0.26)\n",
      "Iter 113: Acc train 93.2% (0.22), acc test 93.0% (0.23)\n",
      "Iter 114: Acc train 92.4% (0.25), acc test 92.2% (0.25)\n",
      "Iter 115: Acc train 93.1% (0.23), acc test 93.0% (0.23)\n",
      "Iter 116: Acc train 92.9% (0.23), acc test 92.9% (0.23)\n",
      "Iter 117: Acc train 93.0% (0.23), acc test 93.2% (0.23)\n",
      "Iter 118: Acc train 93.0% (0.23), acc test 93.3% (0.23)\n",
      "Iter 119: Acc train 92.9% (0.23), acc test 93.0% (0.23)\n",
      "Iter 120: Acc train 93.1% (0.23), acc test 93.0% (0.23)\n",
      "Iter 121: Acc train 93.1% (0.23), acc test 93.2% (0.23)\n",
      "Iter 122: Acc train 92.8% (0.24), acc test 93.0% (0.24)\n",
      "Iter 123: Acc train 92.6% (0.24), acc test 92.7% (0.25)\n",
      "Iter 124: Acc train 93.2% (0.22), acc test 92.8% (0.23)\n",
      "Iter 125: Acc train 92.8% (0.23), acc test 92.8% (0.24)\n",
      "Iter 126: Acc train 92.5% (0.24), acc test 92.5% (0.25)\n",
      "Iter 127: Acc train 93.1% (0.22), acc test 93.0% (0.24)\n",
      "Iter 128: Acc train 93.1% (0.23), acc test 92.8% (0.24)\n",
      "Iter 129: Acc train 93.2% (0.22), acc test 93.2% (0.23)\n",
      "Iter 130: Acc train 93.3% (0.22), acc test 93.3% (0.22)\n",
      "Iter 131: Acc train 93.2% (0.22), acc test 92.9% (0.24)\n",
      "Iter 132: Acc train 93.1% (0.23), acc test 92.9% (0.24)\n",
      "Iter 133: Acc train 93.0% (0.23), acc test 93.0% (0.24)\n",
      "Iter 134: Acc train 93.0% (0.23), acc test 92.8% (0.24)\n",
      "Iter 135: Acc train 93.4% (0.22), acc test 93.3% (0.23)\n",
      "Iter 136: Acc train 93.3% (0.22), acc test 93.6% (0.22)\n",
      "Iter 137: Acc train 93.4% (0.21), acc test 93.1% (0.22)\n",
      "Iter 138: Acc train 93.4% (0.22), acc test 93.0% (0.23)\n",
      "Iter 139: Acc train 93.7% (0.21), acc test 93.4% (0.22)\n",
      "Iter 140: Acc train 93.7% (0.20), acc test 93.5% (0.21)\n",
      "Iter 141: Acc train 93.3% (0.22), acc test 93.2% (0.22)\n",
      "Iter 142: Acc train 93.4% (0.22), acc test 93.2% (0.23)\n",
      "Iter 143: Acc train 92.3% (0.24), acc test 92.2% (0.25)\n",
      "Iter 144: Acc train 92.8% (0.23), acc test 92.9% (0.24)\n",
      "Iter 145: Acc train 92.3% (0.25), acc test 92.5% (0.24)\n",
      "Iter 146: Acc train 93.4% (0.22), acc test 93.3% (0.22)\n",
      "Iter 147: Acc train 93.1% (0.22), acc test 93.1% (0.22)\n",
      "Iter 148: Acc train 93.2% (0.22), acc test 93.1% (0.23)\n",
      "Iter 149: Acc train 93.2% (0.22), acc test 93.4% (0.22)\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "data = MNISTData()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 100\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 100\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "model, loss, optim = init_model(nx, nh, ny, eta)\n",
    "\n",
    "\n",
    "# epoch\n",
    "for iteration in range(150):\n",
    "\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "    Ytrain = torch.argmax(Ytrain, dim=1)\n",
    "\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "\n",
    "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch]\n",
    "\n",
    "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
    "        # en utilisant la fonction loss_accuracy\n",
    "        # effectuer le forward en faisant un appel au modèle \n",
    "        # calculer les gradients avec la fonction backward de l'autograd \n",
    "        # Puis une \"step\" d'optimisation\n",
    "\n",
    "        Yhat = model(X)\n",
    "        L= loss(Yhat, Y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        L.backward()\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "\n",
    "    Yhat_train = model(data.Xtrain)\n",
    "    Yhat_test = model(data.Xtest)\n",
    "    Ytrain = torch.argmax(data.Ytrain, dim=1)\n",
    "    Ytest = torch.argmax(data.Ytest, dim=1)\n",
    "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n",
    "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
    "    print(title) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRoiGbhvmSLO"
   },
   "source": [
    "# Partie 6: Bonus: SVM\n",
    "\n",
    "\n",
    "Entrainez un modèle SVM sur le jeu de data Circles.\n",
    "Le code à remplir est indiqué par des commentaires.\n",
    "\n",
    "\n",
    "Idées : \n",
    "- Essayer d'abord un SVM linéaire (sklearn.svm.LinearSVC dans scikit-learn). Est-ce que cela fonctionne bien ? Pourquoi\n",
    "- Essayer d'autres kernels (possible avec sklearn.svm.SVC).\n",
    "Lequel fonctionne le mieux ? Pourquoi ?\n",
    "- Est-ce que le paramètre C de régularisation à un impact ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 319928,
     "status": "ok",
     "timestamp": 1607744778556,
     "user": {
      "displayName": "Amine Djeghri",
      "photoUrl": "",
      "userId": "06388183709028540430"
     },
     "user_tz": -60
    },
    "id": "VWeW8siymR3g"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "data = CirclesData()\n",
    "Xtrain = data.Xtrain.numpy()\n",
    "Ytrain = data.Ytrain[:, 0].numpy()\n",
    "\n",
    "Xgrid = data.Xgrid.numpy()\n",
    "\n",
    "Xtest = data.Xtest.numpy()\n",
    "Ytest = data.Ytest[:, 0].numpy()\n",
    "\n",
    "def plot_svm_predictions(data, predictions):\n",
    "      plt.figure(2)\n",
    "      plt.clf()\n",
    "      plt.imshow(np.reshape(predictions, (40,40)))\n",
    "      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n",
    "      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n",
    "      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n",
    "      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n",
    "      plt.xlim(0,39)\n",
    "      plt.ylim(0,39)\n",
    "      plt.clim(0.3,0.7)\n",
    "      plt.draw()\n",
    "      plt.pause(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1xcE6zbmXU1"
   },
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "############################\n",
    "### Votre code ici   #######\n",
    "### Entrainer le SVM #######\n",
    "## Voir https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "## et https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "############################\n",
    "\n",
    "svm = None\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgLl7B_3mbOs"
   },
   "outputs": [],
   "source": [
    "## Affichage des résultats\n",
    "\n",
    "Ytest_pred = svm.predict(Xtest)\n",
    "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
    "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
    "Ygrid_pred = svm.predict(Xgrid)\n",
    "plot_svm_predictions(data, Ygrid_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DJEGHRI_MAMOU_TP_3_4_Introduction_aux_réseaux_de_neurones_Sorbonne.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
