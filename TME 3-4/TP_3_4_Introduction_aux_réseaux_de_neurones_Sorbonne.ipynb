{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "TP 3-4 Introduction aux réseaux de neurones - Sorbonne.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbzBJ1m9FBBb"
      },
      "source": [
        "# Attention : \n",
        "# Faire \"File -> Save a copy in Drive\" avant de commencer à modifier le notebook, sinon vos modifications ne seront pas sauvegardées.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfnKy8NB8J5e",
        "outputId": "5d383487-d153-4ffd-b944-312b7a8f3813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!wget http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n",
        "!unzip -j TP3-4.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-30 17:20:07--  http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n",
            "Resolving webia.lip6.fr (webia.lip6.fr)... 132.227.201.33\n",
            "Connecting to webia.lip6.fr (webia.lip6.fr)|132.227.201.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13421167 (13M) [application/zip]\n",
            "Saving to: ‘TP3-4.zip’\n",
            "\n",
            "TP3-4.zip           100%[===================>]  12.80M  11.2MB/s    in 1.1s    \n",
            "\n",
            "2020-09-30 17:20:08 (11.2 MB/s) - ‘TP3-4.zip’ saved [13421167/13421167]\n",
            "\n",
            "Archive:  TP3-4.zip\n",
            "  inflating: tme5.py                 \n",
            "  inflating: mnist.mat               \n",
            "  inflating: circles.py              \n",
            "  inflating: circles.mat             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQ_LLdx8J5b"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%run 'tme5.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48x_ha7f8J5i"
      },
      "source": [
        "# Partie 1 : Forward et Backward manuels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtizX1JV8J5n"
      },
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "    \n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # remplir avec les paramètres Wh, Wy, bh, by\n",
        "    \n",
        "    params[\"Wh\"] = None\n",
        "    params[\"Wy\"] = None\n",
        "    params[\"bh\"] = None\n",
        "    params[\"by\"] = None\n",
        "    \n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk-N_Ny67yo-"
      },
      "source": [
        "def forward(params, X):\n",
        "    \"\"\"\n",
        "    params: dictionnary\n",
        "    X: (n_batch, dimension)\n",
        "    \"\"\"\n",
        "    bsize = X.size(0)\n",
        "    nh = params['Wh'].size(0)\n",
        "    ny = params['Wy'].size(0)\n",
        "    outputs = {}\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # remplir avec les paramètres X, htilde, h, ytilde, yhat\n",
        "    \n",
        "    outputs[\"X\"] = None\n",
        "    outputs[\"htilde\"] = None\n",
        "    outputs[\"h\"] = None\n",
        "    outputs[\"ytilde\"] = None\n",
        "    outputs[\"yhat\"] = None\n",
        "    \n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "    return outputs['yhat'], outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uB0A2b28NZK"
      },
      "source": [
        "def loss_accuracy(Yhat, Y):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "\n",
        "    L = 0\n",
        "    acc = 0\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "    return L, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWJjdiFe8qi5"
      },
      "source": [
        "def backward(params, outputs, Y):\n",
        "    bsize = Y.shape[0]\n",
        "    grads = {}\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # remplir avec les paramètres Wy, Wh, by, bh\n",
        "    \n",
        "    grads[\"Wy\"] = None\n",
        "    grads[\"Wh\"] = None\n",
        "    grads[\"by\"] = None\n",
        "    grads[\"bh\"] = None\n",
        "    \n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAnsISsW9CnH"
      },
      "source": [
        "def sgd(params, grads, eta):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # mettre à jour le contenu de params\n",
        "\n",
        "    params[\"Wh\"] = None\n",
        "    params[\"Wy\"] = None\n",
        "    params[\"bh\"] = None\n",
        "    params[\"by\"] = None    \n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifuW5UFA3DZ"
      },
      "source": [
        "## Algorithme global d'apprentissage (manuel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSw6bd0-qUe"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
        "        # en utilisant les fonctions forward, loss_accuracy, backward, sgd \n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)  \n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHHH5PL8J54"
      },
      "source": [
        "# Partie 2 : Simplification du backward avec `torch.autograd`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G4q5zP0CEvB"
      },
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "    \n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # remplir avec les paramètres Wh, Wy, bh, by\n",
        "    # activer l'autograd sur les poids du réseau\n",
        "    \n",
        "    params[\"Wh\"] = None\n",
        "    params[\"Wy\"] = None\n",
        "    params[\"bh\"] = None\n",
        "    params[\"by\"] = None\n",
        "    \n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0tSjpKCyVB"
      },
      "source": [
        "La fonction `forward` est inchangée par rapport à la partie précédente. \n",
        "\n",
        "La fonction `backward` n'est plus utilisée grâce à l'autograd. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA4ycHlfBzCK"
      },
      "source": [
        "def sgd(params, eta):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # mettre à jour le contenu de params\n",
        "    # attention à bien utiliser torch.no_grad()\n",
        "    # et à remettre les accumulateurs de gradients à zéro\n",
        "\n",
        "    params[\"Wh\"] = None\n",
        "    params[\"Wy\"] = None\n",
        "    params[\"bh\"] = None\n",
        "    params[\"by\"] = None    \n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjgcmgQpDfOb"
      },
      "source": [
        "## Algorithme global d'apprentissage (avec autograd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p5oR3EqDea-"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "  \n",
        "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
        "        # en utilisant les fonctions forward, loss_accuracy, sgd \n",
        "        # calculer les gradients avec la fonction backward de l'autograd \n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)  \n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    # detach() est utilisé pour détacher les predictions du graphes de calcul autograd\n",
        "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FV1iss68J6H"
      },
      "source": [
        "# Partie 3 : Simplification du forward avec `torch.nn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6T5Uq7JEl47"
      },
      "source": [
        "`init_params` et `forward` sont supprimés et remplacés par une fonction `init_model` qui déclare l'architecture du modèle et la loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-h4r-FH8J6I"
      },
      "source": [
        "def init_model(nx, nh, ny):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "\n",
        "    model = None\n",
        "    loss = None\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "    return model, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geE_TI96FXnl"
      },
      "source": [
        "def loss_accuracy(loss, Yhat, Y):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # faire appel à la fonction de loss\n",
        "\n",
        "    L = 0\n",
        "    acc = 0\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "    return L, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93bvFiYGKnA"
      },
      "source": [
        "def sgd(model, eta):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # mettre à jour les paramètres de model\n",
        "    # attention à bien utiliser torch.no_grad()\n",
        "    # et à remettre les accumulateurs de gradients à zéro\n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOxBMmD4Gxtp"
      },
      "source": [
        "## Algorithme global d'apprentissage (avec autograd et les couches `torch.nn`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMBmCNvHCLn"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "model, loss = init_model(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "  \n",
        "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
        "        # en utilisant les fonctions loss_accuracy, sgd\n",
        "        # effectuer le forward en faisant un appel au modèle \n",
        "        # calculer les gradients avec la fonction backward de l'autograd \n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "\n",
        "    Yhat_train = model(data.Xtrain)\n",
        "    Yhat_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)  \n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title) \n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoFSrQNsJCnz"
      },
      "source": [
        "# Partie 4 : Simplification de SGD avec `torch.optim`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8WtN9loJPqP"
      },
      "source": [
        "def init_model(nx, nh, ny, eta):\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "\n",
        "    model = None\n",
        "    loss = None\n",
        "    optim = None\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "    return model, loss, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-0rRzPJYDd"
      },
      "source": [
        "La fonction `sgd` est supprimée. À la place, on fera un appel à `optim.zero_grad()` avant de faire le backward et à `optim.step()` après le backward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82hCupvJxvV"
      },
      "source": [
        "## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9h9nINKJ1LU"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "model, loss, optim = init_model(nx, nh, ny, eta)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    #####################\n",
        "    ## Votre code ici  ##\n",
        "    #####################\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "  \n",
        "        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n",
        "        # en utilisant la fonction loss_accuracy\n",
        "        # effectuer le forward en faisant un appel au modèle \n",
        "        # calculer les gradients avec la fonction backward de l'autograd \n",
        "        # Puis une \"step\" d'optimisation\n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "\n",
        "    Yhat_train = model(data.Xtrain)\n",
        "    Yhat_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)  \n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title) \n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1s4JuOSaZ3"
      },
      "source": [
        "# Partie 5 : MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jly9C4FCSzLP"
      },
      "source": [
        "Reprendre le code d'entraînement précédent en l'appliquant au dataset MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osrFoEr_Syi7"
      },
      "source": [
        "# init\n",
        "data = MNISTData()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 100\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 100\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRoiGbhvmSLO"
      },
      "source": [
        "# Partie 6: Bonus: SVM\n",
        "\n",
        "\n",
        "Entrainez un modèle SVM sur le jeu de data Circles.\n",
        "Le code à remplir est indiqué par des commentaires.\n",
        "\n",
        "\n",
        "Idées : \n",
        "- Essayer d'abord un SVM linéaire (sklearn.svm.LinearSVC dans scikit-learn). Est-ce que cela fonctionne bien ? Pourquoi\n",
        "- Essayer d'autres kernels (possible avec sklearn.svm.SVC).\n",
        "Lequel fonctionne le mieux ? Pourquoi ?\n",
        "- Est-ce que le paramètre C de régularisation à un impact ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWeW8siymR3g"
      },
      "source": [
        "# data\n",
        "data = CirclesData()\n",
        "Xtrain = data.Xtrain.numpy()\n",
        "Ytrain = data.Ytrain[:, 0].numpy()\n",
        "\n",
        "Xgrid = data.Xgrid.numpy()\n",
        "\n",
        "Xtest = data.Xtest.numpy()\n",
        "Ytest = data.Ytest[:, 0].numpy()\n",
        "\n",
        "def plot_svm_predictions(data, predictions):\n",
        "      plt.figure(2)\n",
        "      plt.clf()\n",
        "      plt.imshow(np.reshape(predictions, (40,40)))\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n",
        "      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n",
        "      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n",
        "      plt.xlim(0,39)\n",
        "      plt.ylim(0,39)\n",
        "      plt.clim(0.3,0.7)\n",
        "      plt.draw()\n",
        "      plt.pause(1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xcE6zbmXU1"
      },
      "source": [
        "import sklearn.svm\n",
        "\n",
        "############################\n",
        "### Votre code ici   #######\n",
        "### Entrainer le SVM #######\n",
        "## Voir https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "## et https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "############################\n",
        "\n",
        "svm = None\n",
        "\n",
        "###########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLl7B_3mbOs"
      },
      "source": [
        "## Affichage des résultats\n",
        "\n",
        "Ytest_pred = svm.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
        "Ygrid_pred = svm.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}