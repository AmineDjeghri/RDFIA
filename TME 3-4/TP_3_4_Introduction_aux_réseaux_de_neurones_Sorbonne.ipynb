{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6-final"},"colab":{"name":"TP_3_4_Introduction_aux_réseaux_de_neurones_Sorbonne.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PbzBJ1m9FBBb"},"source":["# Attention : \n","# Faire \"File -> Save a copy in Drive\" avant de commencer à modifier le notebook, sinon vos modifications ne seront pas sauvegardées.\n"]},{"cell_type":"code","metadata":{"id":"NfnKy8NB8J5e","outputId":"5d383487-d153-4ffd-b944-312b7a8f3813","colab":{"base_uri":"https://localhost:8080/","height":302}},"source":["!wget http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n","!unzip -j TP3-4.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-09-30 17:20:07--  http://webia.lip6.fr/~dancette/deep-learning/assets/TP3-4/TP3-4.zip\n","Resolving webia.lip6.fr (webia.lip6.fr)... 132.227.201.33\n","Connecting to webia.lip6.fr (webia.lip6.fr)|132.227.201.33|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13421167 (13M) [application/zip]\n","Saving to: ‘TP3-4.zip’\n","\n","TP3-4.zip           100%[===================>]  12.80M  11.2MB/s    in 1.1s    \n","\n","2020-09-30 17:20:08 (11.2 MB/s) - ‘TP3-4.zip’ saved [13421167/13421167]\n","\n","Archive:  TP3-4.zip\n","  inflating: tme5.py                 \n","  inflating: mnist.mat               \n","  inflating: circles.py              \n","  inflating: circles.mat             \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"54lNh5hoRv3T","executionInfo":{"status":"ok","timestamp":1605185202851,"user_tz":-60,"elapsed":40188,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"ce27bded-557e-4158-907c-c719f9c59f43","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(\"/content/gdrive/My Drive/git/RDFIA/TME 3-4\")\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"," circles.mat\n"," circles.py\n","'Compte rendu TME3-4 DJEGHRI_MAMOU.docx'\n"," mnist.mat\n"," tme5.py\n"," TP_3_4_Introduction_aux_réseaux_de_neurones_Sorbonne.ipynb\n"," tp3-4.pdf\n","'tp3-4 solutions gradient.pdf'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2vQ_LLdx8J5b","executionInfo":{"status":"ok","timestamp":1605189510061,"user_tz":-60,"elapsed":583,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"7652def2-eea4-498f-9719-46a98cb3e288","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import math\n","import torch\n","from torch.autograd import Variable\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%run tme5.py"],"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"48x_ha7f8J5i"},"source":["# Partie 1 : Forward et Backward manuels"]},{"cell_type":"code","metadata":{"id":"GtizX1JV8J5n","executionInfo":{"status":"ok","timestamp":1605189511689,"user_tz":-60,"elapsed":1047,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def init_params(nx, nh, ny):\n","    \"\"\"\n","    nx, nh, ny: integers\n","    out params: dictionnary\n","    \"\"\"\n","    params = {}\n","    \n","    # remplir avec les paramètres Wh, Wy, bh, by\n","\n","    params[\"Wh\"] = torch.normal(mean=0, std=0.3, size=(nh,nx))\n","                # or torch.randn(nh, nx).normal_(mean=0,std=0.3)\n","    params[\"Wy\"] = torch.normal(mean=0, std=0.3, size=(ny,nh))\n","    params[\"bh\"] = torch.normal(mean=0, std=0.3, size=(nh,))\n","    params[\"by\"] = torch.normal(mean=0, std=0.3, size=(ny,))\n","    \n","    return params"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"jk-N_Ny67yo-","executionInfo":{"status":"ok","timestamp":1605189511696,"user_tz":-60,"elapsed":771,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def forward(params, X):\n","    \"\"\"\n","    params: dictionnary\n","    X: (n_batch, dimension)\n","    \"\"\"\n","    bsize = X.size(0)\n","    nh = params['Wh'].size(0)\n","    ny = params['Wy'].size(0)\n","    outputs = {}\n","\n","\n","    # remplir avec les paramètres X, htilde, h, ytilde, yhat\n","    \n","    outputs[\"X\"] = X\n","    outputs[\"htilde\"] = torch.mm(X,params[\"Wh\"].T) + params[\"bh\"]\n","    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n","    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"],params[\"Wy\"].T) + params[\"by\"]\n","    outputs[\"yhat\"] = torch.exp(outputs[\"ytilde\"]) / torch.sum(torch.exp(outputs[\"ytilde\"]), dim=1, keepdim=True)\n","\n","    return outputs['yhat'], outputs"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"-uB0A2b28NZK","executionInfo":{"status":"ok","timestamp":1605189512393,"user_tz":-60,"elapsed":1197,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def loss_accuracy(Yhat, Y):\n","\n","\n","    L = 0\n","    acc = 0\n","\n","    L = -torch.mean(torch.sum(Y*torch.log(Yhat)))\n","\n","    _, indsY = torch.max(Y, 1)\n","    _, indsYhat = torch.max(Yhat, 1)\n","    acc = (indsY == indsYhat).float().mean()*100\n","\n","    return L, acc"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWJjdiFe8qi5","executionInfo":{"status":"ok","timestamp":1605189512396,"user_tz":-60,"elapsed":847,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def backward(params, outputs, Y):\n","    bsize = Y.shape[0]\n","    grads = {}\n","\n","    # remplir avec les paramètres Wy, Wh, by, bh\n","\n","    # dérivé de L par rapport à ytilde\n","    grad_L_ytilde = outputs['yhat'] - Y \n","\n","    # dérivé de ytilde par rapport à Wh\n","    grad_ytilde_Wh = outputs['h']\n","    # dérivé de ytilde par rapport à Bh\n","    grad_ytilde_bh = 1\n","\n","\n","    grads[\"Wy\"] = torch.mm(grad_L_ytilde.T, grad_ytilde_Wh)   # dérivé de L par rapport à Wy\n","    grads[\"by\"] = torch.sum(grad_L_ytilde, dim=0)   # dérivé de L par rapport à by\n","\n","    # dérivé de y_tilde par rapport à h\n","    grad_ytilde_h = params['Wy']\n","    # dérivé de L par rapport à htilde\n","    grad_h_htilde = torch.mm(grad_L_ytilde,grad_ytilde_h) * (1-torch.pow(grad_ytilde_Wh,2))\n","\n","    grads[\"Wh\"] = torch.mm(grad_h_htilde, outputs['X'])  # dérivé de L par rapport à Wh\n","    \n","    grads[\"bh\"] = torch.sum(grad_h_htilde, dim=0)  # dérivé de L par rapport à bh\n","\n","    return grads"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAnsISsW9CnH","executionInfo":{"status":"ok","timestamp":1605189513381,"user_tz":-60,"elapsed":1104,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def sgd(params, grads, eta):\n","    params[\"Wh\"] = params[\"Wh\"] - eta * grads['Wh']  \n","    params[\"Wy\"] = params[\"Wy\"] - eta * grads['Wy']  \n","    params[\"bh\"] = params[\"bh\"] - eta * grads['bh']  \n","    params[\"by\"] = params[\"by\"] - eta * grads['by']    \n","\n","    return params"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hifuW5UFA3DZ"},"source":["## Algorithme global d'apprentissage (manuel)"]},{"cell_type":"code","metadata":{"id":"4RSw6bd0-qUe","executionInfo":{"status":"ok","timestamp":1605189543175,"user_tz":-60,"elapsed":28955,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"07cca954-9f9b-4345-f994-b59c51209e14","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1SdJ2bgPBLKZDg_O0JVMw-klEsxIzF_a5"}},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 10\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","\n","params = init_params(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","\n","        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n","        # en utilisant les fonctions forward, loss_accuracy, backward, sgd \n","\n","        Yhat, outputs = forward(params, X)\n","        loss, acc = loss_accuracy(Yhat, Y)\n","        grads = backward(params, outputs, Y) \n","        params = sgd(params, grads, eta)\n","\n","    Yhat_train, _ = forward(params, data.Xtrain)\n","    Yhat_test, _ = forward(params, data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n","    Ygrid, _ = forward(params, data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title)\n","    data.plot_data_with_grid(Ygrid, title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"OrHHH5PL8J54"},"source":["# Partie 2 : Simplification du backward avec `torch.autograd`\n","\n"]},{"cell_type":"code","metadata":{"id":"7G4q5zP0CEvB","executionInfo":{"status":"ok","timestamp":1605189558745,"user_tz":-60,"elapsed":1063,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def init_params(nx, nh, ny):\n","    \"\"\"\n","    nx, nh, ny: integers\n","    out params: dictionnary\n","    \"\"\"\n","    params = {}\n","    \n","\n","    # remplir avec les paramètres Wh, Wy, bh, by\n","    # activer l'autograd sur les poids du réseau\n","    \n","    params[\"Wh\"] = torch.normal(mean=0, std=0.3, size=(nh, nx), requires_grad=True)\n","    params[\"Wy\"] = torch.normal(mean=0, std=0.3, size=(ny, nh), requires_grad=True)\n","    params[\"bh\"] = torch.normal(mean=0, std=0.3, size=(nh,), requires_grad=True)\n","    params[\"by\"] = torch.normal(mean=0, std=0.3, size=(ny,), requires_grad=True)\n","    \n","\n","    return params"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZL0tSjpKCyVB"},"source":["La fonction `forward` est inchangée par rapport à la partie précédente. \n","\n","La fonction `backward` n'est plus utilisée grâce à l'autograd. "]},{"cell_type":"code","metadata":{"id":"hA4ycHlfBzCK","executionInfo":{"status":"ok","timestamp":1605189558749,"user_tz":-60,"elapsed":659,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def sgd(params, eta):\n","\n","    # mettre à jour le contenu de params\n","    # attention à bien utiliser torch.no_grad()\n","    # et à remettre les accumulateurs de gradients à zéro\n","    with torch.no_grad():\n","        for param in params.values():\n","          param-= eta*param.grad\n","          param.grad.zero_()\n","\n","\n","    return params"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjgcmgQpDfOb"},"source":["## Algorithme global d'apprentissage (avec autograd)"]},{"cell_type":"code","metadata":{"id":"8p5oR3EqDea-","executionInfo":{"status":"ok","timestamp":1605189589891,"user_tz":-60,"elapsed":30816,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"586ad2e9-e858-4cdf-f77f-9cde8f9e5744","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"18sD-yfK86PKtHais7SKl49bFWIT7EBjT"}},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 10\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","\n","params = init_params(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","  \n","        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n","        # en utilisant les fonctions forward, loss_accuracy, sgd \n","        # calculer les gradients avec la fonction backward de l'autograd \n","\n","        Yhat, outputs = forward(params, X)\n","        loss, acc = loss_accuracy(Yhat, Y)\n","        loss.backward()\n","        params = sgd(params, eta)\n","\n","\n","\n","    Yhat_train, _ = forward(params, data.Xtrain)\n","    Yhat_test, _ = forward(params, data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n","    Ygrid, _ = forward(params, data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title)\n","    # detach() est utilisé pour détacher les predictions du graphes de calcul autograd\n","    data.plot_data_with_grid(Ygrid.detach(), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"5FV1iss68J6H"},"source":["# Partie 3 : Simplification du forward avec `torch.nn`"]},{"cell_type":"markdown","metadata":{"id":"x6T5Uq7JEl47"},"source":["`init_params` et `forward` sont supprimés et remplacés par une fonction `init_model` qui déclare l'architecture du modèle et la loss."]},{"cell_type":"code","metadata":{"id":"5-h4r-FH8J6I","executionInfo":{"status":"ok","timestamp":1605197697097,"user_tz":-60,"elapsed":582,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def init_model(nx, nh, ny):\n","\n","    model = torch.nn.Sequential(\n","        torch.nn.Linear(nx, nh),\n","        torch.nn.Tanh(),\n","        torch.nn.Linear(nh, ny),\n","    )\n","    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class \n","    loss = torch.nn.CrossEntropyLoss()\n","\n","    return model, loss"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"geE_TI96FXnl","executionInfo":{"status":"ok","timestamp":1605198169198,"user_tz":-60,"elapsed":1193,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def loss_accuracy(loss, Yhat, Y):\n","\n","    # faire appel à la fonction de loss\n","\n","    L = loss(Yhat, Y)\n","    _, indsYhat = torch.max(Yhat, 1)\n","    acc = (Y == indsYhat).float().mean()*100\n","\n","    return L, acc"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"e93bvFiYGKnA","executionInfo":{"status":"ok","timestamp":1605198169646,"user_tz":-60,"elapsed":1305,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def sgd(model, eta):\n","\n","    # mettre à jour les paramètres de model\n","    # attention à bien utiliser torch.no_grad()\n","    # et à remettre les accumulateurs de gradients à zéro\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= eta * param.grad\n","        model.zero_grad()\n","\n","    return model"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOxBMmD4Gxtp"},"source":["## Algorithme global d'apprentissage (avec autograd et les couches `torch.nn`)"]},{"cell_type":"code","metadata":{"id":"4hMBmCNvHCLn","executionInfo":{"status":"ok","timestamp":1605199147361,"user_tz":-60,"elapsed":29700,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"1b699df7-864a-41c7-c760-b04a4883799e","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ldvFVoCcKT1k7UQGCDSCbrxQea2FVgCn"}},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 10\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","\n","model, loss = init_model(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","    Ytrain = torch.argmax(Ytrain, dim=1)\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch]\n","  \n","        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n","        # en utilisant les fonctions loss_accuracy, sgd\n","        # effectuer le forward en faisant un appel au modèle \n","        # calculer les gradients avec la fonction backward de l'autograd \n","\n","        Yhat = model(X)\n","        l, acc = loss_accuracy(loss, Yhat, Y)\n","        l.backward()\n","        sgd(model, eta)\n","\n","\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","\n","    Ytrain = torch.argmax(data.Ytrain, dim=1)\n","    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n","    Ytest = torch.argmax(data.Ytest, dim=1)\n","    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n","    Ygrid = model(data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title) \n","    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"GoFSrQNsJCnz"},"source":["# Partie 4 : Simplification de SGD avec `torch.optim`"]},{"cell_type":"code","metadata":{"id":"S8WtN9loJPqP","executionInfo":{"status":"ok","timestamp":1605199624571,"user_tz":-60,"elapsed":531,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}}},"source":["def init_model(nx, nh, ny, eta):\n","  \n","    model = torch.nn.Sequential(\n","        torch.nn.Linear(nx, nh),\n","        torch.nn.Tanh(),\n","        torch.nn.Linear(nh, ny),\n","    )\n","    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class \n","    loss = torch.nn.CrossEntropyLoss()\n","    optim = torch.optim.SGD(model.parameters(), lr=eta)\n","\n","    return model, loss, optim"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eY-0rRzPJYDd"},"source":["La fonction `sgd` est supprimée. À la place, on fera un appel à `optim.zero_grad()` avant de faire le backward et à `optim.step()` après le backward. "]},{"cell_type":"markdown","metadata":{"id":"q82hCupvJxvV"},"source":["## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"]},{"cell_type":"code","metadata":{"id":"V9h9nINKJ1LU","executionInfo":{"status":"ok","timestamp":1605200291440,"user_tz":-60,"elapsed":29584,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"e89b4a8e-7e8b-4fb5-d089-2aa6e2e09be1","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ps4AQyMZr_QoiSDB2EDNrf7KGLyu9WiI"}},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 10\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","\n","model, loss, optim = init_model(nx, nh, ny, eta)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","    Ytrain = torch.argmax(Ytrain, dim=1)\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch]\n","\n","        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n","        # en utilisant la fonction loss_accuracy\n","        # effectuer le forward en faisant un appel au modèle \n","        # calculer les gradients avec la fonction backward de l'autograd \n","        # Puis une \"step\" d'optimisation\n","\n","        Yhat = model(X)\n","        L= loss(Yhat, Y)\n","\n","        optim.zero_grad()\n","        L.backward()\n","        optim.step()\n","        \n","\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","    Ytrain = torch.argmax(data.Ytrain, dim=1)\n","    Ytest = torch.argmax(data.Ytest, dim=1)\n","    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n","    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n","    Ygrid = model(data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title) \n","    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Ts1s4JuOSaZ3"},"source":["# Partie 5 : MNIST"]},{"cell_type":"markdown","metadata":{"id":"jly9C4FCSzLP"},"source":["Reprendre le code d'entraînement précédent en l'appliquant au dataset MNIST."]},{"cell_type":"code","metadata":{"id":"osrFoEr_Syi7","executionInfo":{"status":"ok","timestamp":1605200734260,"user_tz":-60,"elapsed":279925,"user":{"displayName":"Amine Djeghri","photoUrl":"","userId":"06388183709028540430"}},"outputId":"6f378564-2f93-45bf-e1e4-8b3c985d8207","colab":{"base_uri":"https://localhost:8080/"}},"source":["# init\n","data = MNISTData()\n","N = data.Xtrain.shape[0]\n","Nbatch = 100\n","nx = data.Xtrain.shape[1]\n","nh = 100\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","\n","model, loss, optim = init_model(nx, nh, ny, eta)\n","\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","    Ytrain = torch.argmax(Ytrain, dim=1)\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch]\n","\n","        # écrire l'algorithme d'apprentissage sur le batch (X,Y)\n","        # en utilisant la fonction loss_accuracy\n","        # effectuer le forward en faisant un appel au modèle \n","        # calculer les gradients avec la fonction backward de l'autograd \n","        # Puis une \"step\" d'optimisation\n","\n","        Yhat = model(X)\n","        L= loss(Yhat, Y)\n","\n","        optim.zero_grad()\n","        L.backward()\n","        optim.step()\n","        \n","\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","    Ytrain = torch.argmax(data.Ytrain, dim=1)\n","    Ytest = torch.argmax(data.Ytest, dim=1)\n","    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, Ytrain)\n","    Ltest, acctest = loss_accuracy(loss, Yhat_test, Ytest)\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title) "],"execution_count":83,"outputs":[{"output_type":"stream","text":["Iter 0: Acc train 85.0% (0.53), acc test 85.9% (0.51)\n","Iter 1: Acc train 85.8% (0.49), acc test 86.7% (0.46)\n","Iter 2: Acc train 89.0% (0.38), acc test 89.4% (0.37)\n","Iter 3: Acc train 88.5% (0.39), acc test 88.8% (0.38)\n","Iter 4: Acc train 88.4% (0.40), acc test 89.1% (0.39)\n","Iter 5: Acc train 88.2% (0.39), acc test 89.1% (0.37)\n","Iter 6: Acc train 88.1% (0.40), acc test 88.5% (0.39)\n","Iter 7: Acc train 88.3% (0.38), acc test 88.4% (0.37)\n","Iter 8: Acc train 89.0% (0.36), acc test 89.5% (0.35)\n","Iter 9: Acc train 88.6% (0.37), acc test 88.7% (0.36)\n","Iter 10: Acc train 89.1% (0.37), acc test 89.4% (0.36)\n","Iter 11: Acc train 89.2% (0.36), acc test 90.0% (0.33)\n","Iter 12: Acc train 89.6% (0.35), acc test 89.6% (0.34)\n","Iter 13: Acc train 89.0% (0.37), acc test 89.3% (0.35)\n","Iter 14: Acc train 88.9% (0.36), acc test 89.4% (0.36)\n","Iter 15: Acc train 89.7% (0.34), acc test 89.9% (0.34)\n","Iter 16: Acc train 88.6% (0.37), acc test 88.9% (0.36)\n","Iter 17: Acc train 88.6% (0.37), acc test 88.5% (0.37)\n","Iter 18: Acc train 88.9% (0.37), acc test 89.2% (0.36)\n","Iter 19: Acc train 89.2% (0.36), acc test 89.3% (0.36)\n","Iter 20: Acc train 89.5% (0.35), acc test 89.6% (0.34)\n","Iter 21: Acc train 90.0% (0.33), acc test 89.9% (0.33)\n","Iter 22: Acc train 89.7% (0.34), acc test 90.2% (0.33)\n","Iter 23: Acc train 90.8% (0.31), acc test 91.4% (0.30)\n","Iter 24: Acc train 90.5% (0.32), acc test 91.2% (0.30)\n","Iter 25: Acc train 90.0% (0.33), acc test 90.4% (0.32)\n","Iter 26: Acc train 89.5% (0.34), acc test 90.0% (0.33)\n","Iter 27: Acc train 90.2% (0.32), acc test 90.5% (0.31)\n","Iter 28: Acc train 90.4% (0.31), acc test 90.5% (0.31)\n","Iter 29: Acc train 90.0% (0.33), acc test 89.6% (0.33)\n","Iter 30: Acc train 90.2% (0.32), acc test 90.5% (0.31)\n","Iter 31: Acc train 91.2% (0.29), acc test 91.0% (0.30)\n","Iter 32: Acc train 90.2% (0.32), acc test 90.6% (0.31)\n","Iter 33: Acc train 90.7% (0.31), acc test 90.5% (0.31)\n","Iter 34: Acc train 91.3% (0.29), acc test 90.8% (0.29)\n","Iter 35: Acc train 91.0% (0.30), acc test 90.8% (0.30)\n","Iter 36: Acc train 90.0% (0.32), acc test 89.9% (0.32)\n","Iter 37: Acc train 90.9% (0.30), acc test 90.8% (0.30)\n","Iter 38: Acc train 91.1% (0.29), acc test 91.3% (0.29)\n","Iter 39: Acc train 91.0% (0.29), acc test 91.0% (0.30)\n","Iter 40: Acc train 91.0% (0.29), acc test 91.2% (0.29)\n","Iter 41: Acc train 91.7% (0.28), acc test 91.7% (0.28)\n","Iter 42: Acc train 90.8% (0.31), acc test 91.3% (0.29)\n","Iter 43: Acc train 91.2% (0.29), acc test 91.4% (0.29)\n","Iter 44: Acc train 90.6% (0.31), acc test 90.8% (0.31)\n","Iter 45: Acc train 91.4% (0.28), acc test 91.6% (0.28)\n","Iter 46: Acc train 91.1% (0.29), acc test 91.1% (0.29)\n","Iter 47: Acc train 91.5% (0.28), acc test 91.6% (0.28)\n","Iter 48: Acc train 92.4% (0.25), acc test 92.5% (0.25)\n","Iter 49: Acc train 91.2% (0.29), acc test 91.5% (0.28)\n","Iter 50: Acc train 91.7% (0.27), acc test 91.9% (0.27)\n","Iter 51: Acc train 91.6% (0.27), acc test 91.8% (0.27)\n","Iter 52: Acc train 91.5% (0.28), acc test 91.4% (0.28)\n","Iter 53: Acc train 92.0% (0.26), acc test 92.1% (0.26)\n","Iter 54: Acc train 91.7% (0.27), acc test 92.0% (0.26)\n","Iter 55: Acc train 92.7% (0.24), acc test 92.4% (0.24)\n","Iter 56: Acc train 92.4% (0.25), acc test 91.9% (0.26)\n","Iter 57: Acc train 92.0% (0.26), acc test 91.8% (0.27)\n","Iter 58: Acc train 91.9% (0.27), acc test 91.8% (0.27)\n","Iter 59: Acc train 91.8% (0.27), acc test 91.5% (0.27)\n","Iter 60: Acc train 92.1% (0.26), acc test 92.0% (0.27)\n","Iter 61: Acc train 92.2% (0.26), acc test 91.9% (0.27)\n","Iter 62: Acc train 92.1% (0.26), acc test 91.8% (0.27)\n","Iter 63: Acc train 92.6% (0.24), acc test 92.5% (0.25)\n","Iter 64: Acc train 92.4% (0.25), acc test 92.1% (0.26)\n","Iter 65: Acc train 91.4% (0.28), acc test 91.8% (0.28)\n","Iter 66: Acc train 92.3% (0.25), acc test 92.3% (0.25)\n","Iter 67: Acc train 92.6% (0.24), acc test 92.7% (0.24)\n","Iter 68: Acc train 92.7% (0.24), acc test 92.6% (0.24)\n","Iter 69: Acc train 91.9% (0.27), acc test 92.0% (0.26)\n","Iter 70: Acc train 92.2% (0.26), acc test 92.4% (0.26)\n","Iter 71: Acc train 92.0% (0.26), acc test 92.1% (0.26)\n","Iter 72: Acc train 92.2% (0.26), acc test 92.7% (0.25)\n","Iter 73: Acc train 92.6% (0.25), acc test 92.8% (0.25)\n","Iter 74: Acc train 92.8% (0.24), acc test 93.2% (0.24)\n","Iter 75: Acc train 92.0% (0.26), acc test 91.9% (0.26)\n","Iter 76: Acc train 92.6% (0.25), acc test 92.4% (0.25)\n","Iter 77: Acc train 92.5% (0.25), acc test 92.6% (0.24)\n","Iter 78: Acc train 92.8% (0.23), acc test 92.7% (0.24)\n","Iter 79: Acc train 93.2% (0.22), acc test 92.9% (0.22)\n","Iter 80: Acc train 93.0% (0.23), acc test 92.7% (0.24)\n","Iter 81: Acc train 92.5% (0.25), acc test 92.2% (0.25)\n","Iter 82: Acc train 92.9% (0.24), acc test 92.9% (0.24)\n","Iter 83: Acc train 92.7% (0.24), acc test 92.6% (0.24)\n","Iter 84: Acc train 93.0% (0.23), acc test 93.0% (0.23)\n","Iter 85: Acc train 93.1% (0.23), acc test 92.8% (0.24)\n","Iter 86: Acc train 92.9% (0.23), acc test 93.0% (0.24)\n","Iter 87: Acc train 92.4% (0.25), acc test 92.3% (0.25)\n","Iter 88: Acc train 92.6% (0.24), acc test 92.8% (0.24)\n","Iter 89: Acc train 93.4% (0.22), acc test 93.4% (0.23)\n","Iter 90: Acc train 93.0% (0.23), acc test 92.9% (0.22)\n","Iter 91: Acc train 93.0% (0.23), acc test 93.0% (0.23)\n","Iter 92: Acc train 93.4% (0.22), acc test 93.4% (0.22)\n","Iter 93: Acc train 93.8% (0.21), acc test 93.8% (0.21)\n","Iter 94: Acc train 93.3% (0.23), acc test 92.9% (0.24)\n","Iter 95: Acc train 93.2% (0.23), acc test 92.8% (0.24)\n","Iter 96: Acc train 93.3% (0.22), acc test 93.0% (0.23)\n","Iter 97: Acc train 93.2% (0.22), acc test 93.2% (0.23)\n","Iter 98: Acc train 93.1% (0.22), acc test 93.0% (0.23)\n","Iter 99: Acc train 93.5% (0.22), acc test 93.2% (0.22)\n","Iter 100: Acc train 93.6% (0.21), acc test 93.5% (0.22)\n","Iter 101: Acc train 93.4% (0.22), acc test 93.5% (0.22)\n","Iter 102: Acc train 93.7% (0.21), acc test 93.6% (0.21)\n","Iter 103: Acc train 93.1% (0.23), acc test 93.3% (0.23)\n","Iter 104: Acc train 93.1% (0.22), acc test 93.1% (0.23)\n","Iter 105: Acc train 92.8% (0.24), acc test 92.6% (0.25)\n","Iter 106: Acc train 92.7% (0.24), acc test 92.4% (0.24)\n","Iter 107: Acc train 93.0% (0.23), acc test 92.8% (0.23)\n","Iter 108: Acc train 93.3% (0.22), acc test 93.1% (0.23)\n","Iter 109: Acc train 93.4% (0.22), acc test 93.0% (0.23)\n","Iter 110: Acc train 93.1% (0.22), acc test 92.6% (0.24)\n","Iter 111: Acc train 92.5% (0.25), acc test 92.5% (0.25)\n","Iter 112: Acc train 92.7% (0.24), acc test 92.8% (0.24)\n","Iter 113: Acc train 92.8% (0.23), acc test 92.9% (0.24)\n","Iter 114: Acc train 93.0% (0.23), acc test 93.1% (0.23)\n","Iter 115: Acc train 93.4% (0.22), acc test 93.2% (0.22)\n","Iter 116: Acc train 92.5% (0.24), acc test 92.3% (0.25)\n","Iter 117: Acc train 93.0% (0.23), acc test 92.7% (0.23)\n","Iter 118: Acc train 93.0% (0.23), acc test 93.0% (0.23)\n","Iter 119: Acc train 92.5% (0.24), acc test 92.6% (0.24)\n","Iter 120: Acc train 93.4% (0.21), acc test 93.3% (0.22)\n","Iter 121: Acc train 93.2% (0.22), acc test 93.3% (0.22)\n","Iter 122: Acc train 93.6% (0.21), acc test 93.4% (0.22)\n","Iter 123: Acc train 93.3% (0.21), acc test 93.4% (0.22)\n","Iter 124: Acc train 93.1% (0.22), acc test 92.8% (0.23)\n","Iter 125: Acc train 93.4% (0.22), acc test 93.1% (0.22)\n","Iter 126: Acc train 93.4% (0.21), acc test 93.2% (0.22)\n","Iter 127: Acc train 93.8% (0.20), acc test 93.6% (0.21)\n","Iter 128: Acc train 93.9% (0.20), acc test 93.8% (0.20)\n","Iter 129: Acc train 93.7% (0.21), acc test 93.4% (0.22)\n","Iter 130: Acc train 93.2% (0.22), acc test 92.9% (0.23)\n","Iter 131: Acc train 93.4% (0.21), acc test 93.4% (0.21)\n","Iter 132: Acc train 93.4% (0.22), acc test 93.5% (0.22)\n","Iter 133: Acc train 93.6% (0.21), acc test 93.4% (0.22)\n","Iter 134: Acc train 93.2% (0.22), acc test 92.9% (0.23)\n","Iter 135: Acc train 93.4% (0.21), acc test 93.1% (0.23)\n","Iter 136: Acc train 93.4% (0.22), acc test 93.2% (0.22)\n","Iter 137: Acc train 93.8% (0.20), acc test 93.7% (0.21)\n","Iter 138: Acc train 93.9% (0.20), acc test 93.7% (0.21)\n","Iter 139: Acc train 93.4% (0.21), acc test 93.0% (0.22)\n","Iter 140: Acc train 93.5% (0.21), acc test 93.2% (0.22)\n","Iter 141: Acc train 92.9% (0.23), acc test 92.9% (0.23)\n","Iter 142: Acc train 93.6% (0.21), acc test 93.4% (0.22)\n","Iter 143: Acc train 93.7% (0.20), acc test 93.5% (0.21)\n","Iter 144: Acc train 93.6% (0.21), acc test 93.4% (0.22)\n","Iter 145: Acc train 93.3% (0.21), acc test 93.0% (0.23)\n","Iter 146: Acc train 93.7% (0.21), acc test 93.6% (0.21)\n","Iter 147: Acc train 93.8% (0.20), acc test 93.5% (0.21)\n","Iter 148: Acc train 93.6% (0.21), acc test 93.6% (0.22)\n","Iter 149: Acc train 94.0% (0.20), acc test 93.9% (0.20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YRoiGbhvmSLO"},"source":["# Partie 6: Bonus: SVM\n","\n","\n","Entrainez un modèle SVM sur le jeu de data Circles.\n","Le code à remplir est indiqué par des commentaires.\n","\n","\n","Idées : \n","- Essayer d'abord un SVM linéaire (sklearn.svm.LinearSVC dans scikit-learn). Est-ce que cela fonctionne bien ? Pourquoi\n","- Essayer d'autres kernels (possible avec sklearn.svm.SVC).\n","Lequel fonctionne le mieux ? Pourquoi ?\n","- Est-ce que le paramètre C de régularisation à un impact ? Pourquoi ?"]},{"cell_type":"code","metadata":{"id":"VWeW8siymR3g"},"source":["# data\n","data = CirclesData()\n","Xtrain = data.Xtrain.numpy()\n","Ytrain = data.Ytrain[:, 0].numpy()\n","\n","Xgrid = data.Xgrid.numpy()\n","\n","Xtest = data.Xtest.numpy()\n","Ytest = data.Ytest[:, 0].numpy()\n","\n","def plot_svm_predictions(data, predictions):\n","      plt.figure(2)\n","      plt.clf()\n","      plt.imshow(np.reshape(predictions, (40,40)))\n","      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n","      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n","      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n","      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n","      plt.xlim(0,39)\n","      plt.ylim(0,39)\n","      plt.clim(0.3,0.7)\n","      plt.draw()\n","      plt.pause(1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1xcE6zbmXU1"},"source":["import sklearn.svm\n","\n","############################\n","### Votre code ici   #######\n","### Entrainer le SVM #######\n","## Voir https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n","## et https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","############################\n","\n","svm = None\n","\n","###########################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgLl7B_3mbOs"},"source":["## Affichage des résultats\n","\n","Ytest_pred = svm.predict(Xtest)\n","accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n","print(f\"Accuracy : {100 * accuracy:.2f}\")\n","Ygrid_pred = svm.predict(Xgrid)\n","plot_svm_predictions(data, Ygrid_pred)"],"execution_count":null,"outputs":[]}]}