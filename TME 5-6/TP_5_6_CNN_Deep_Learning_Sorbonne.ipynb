{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP 5-6 - CNN-  Deep Learning - Sorbonne ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWb11WT-07r"
      },
      "source": [
        "\n",
        "## Avant de commencer le TP, \n",
        "- vérifiez que vous êtes sur un environnement GPU et python 3 : \n",
        "  \n",
        "  Éxecution -> Modifier le type d'éxecution -> Type d'éxecution = python2, Accélerateur matériel = GPU\n",
        "\n",
        "- Fichier -> Sauvegarder une copie dans mon drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac5Js1iML3d_"
      },
      "source": [
        "!git clone https://github.com/cdancette/deep-learning-polytech-tp6-7.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTv6tOOdOCr9"
      },
      "source": [
        "cd deep-learning-polytech-tp6-7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7lgAEJvPCh"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from tme6 import *\n",
        "\n",
        "PRINT_INTERVAL = 200\n",
        "PATH=\"datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xVkUxvwy6a0"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Cette classe contient la structure du réseau de neurones\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # On défini d'abord les couches de convolution et de pooling comme un\n",
        "        # groupe de couches `self.features`\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, (5, 5), stride=1, padding=2),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # On défini les couches fully connected comme un groupe de couches\n",
        "        # `self.classifier`\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(400, 120),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84, 10)\n",
        "            # Rappel : Le softmax est inclus dans la loss, ne pas le mettre ici\n",
        "        )\n",
        "\n",
        "    # méthode appelée quand on applique le réseau à un batch d'input\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # taille du batch\n",
        "        output = self.features(input) # on calcule la sortie des conv\n",
        "        output = output.view(bsize, -1) # on applati les feature map 2D en un\n",
        "                                        # vecteur 1D pour chaque input\n",
        "        output = self.classifier(output) # on calcule la sortie des fc\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    Cette fonction charge le dataset et effectue des transformations sur chaqu\n",
        "    image (listées dans `transform=...`).\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.MNIST(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "    val_dataset = datasets.MNIST(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "def epoch(data, model, criterion, optimizer=None, cuda=False):\n",
        "    \"\"\"\n",
        "    Fait une passe (appelée epoch en anglais) sur les données `data` avec le\n",
        "    modèle `model`. Evalue `criterion` comme loss.\n",
        "    Si `optimizer` est fourni, effectue une epoch d'apprentissage en utilisant\n",
        "    l'optimiseur donné, sinon, effectue une epoch d'évaluation (pas de backward)\n",
        "    du modèle.\n",
        "    \"\"\"\n",
        "\n",
        "    # indique si le modele est en mode eval ou train (certaines couches se\n",
        "    # comportent différemment en train et en eval)\n",
        "    model.eval() if optimizer is None else model.train()\n",
        "\n",
        "    # objets pour stocker les moyennes des metriques\n",
        "    avg_loss = AverageMeter()\n",
        "    avg_top1_acc = AverageMeter()\n",
        "    avg_top5_acc = AverageMeter()\n",
        "    avg_batch_time = AverageMeter()\n",
        "    global loss_plot\n",
        "\n",
        "    # on itere sur les batchs du dataset\n",
        "    tic = time.time()\n",
        "    for i, (input, target) in enumerate(data):\n",
        "\n",
        "        if cuda: # si on fait du GPU, passage en CUDA\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # forward\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backward si on est en \"train\"\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # calcul des metriques\n",
        "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "        batch_time = time.time() - tic\n",
        "        tic = time.time()\n",
        "\n",
        "        # mise a jour des moyennes\n",
        "        avg_loss.update(loss.item())\n",
        "        avg_top1_acc.update(prec1.item())\n",
        "        avg_top5_acc.update(prec5.item())\n",
        "        avg_batch_time.update(batch_time)\n",
        "        if optimizer:\n",
        "            loss_plot.update(avg_loss.val)\n",
        "        # affichage des infos\n",
        "        if i % PRINT_INTERVAL == 0:\n",
        "            print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
        "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
        "                  'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
        "                   \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
        "                   top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "            if optimizer:\n",
        "                loss_plot.plot()\n",
        "\n",
        "    # Affichage des infos sur l'epoch\n",
        "    print('\\n===============> Total time {batch_time:d}s\\t'\n",
        "          'Avg loss {loss.avg:.4f}\\t'\n",
        "          'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
        "          'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
        "           batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
        "           top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "\n",
        "    return avg_top1_acc, avg_top5_acc, avg_loss\n",
        "\n",
        "\n",
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # ex de params :\n",
        "    #   {\"batch_size\": 128, \"epochs\": 5, \"lr\": 0.1}\n",
        "    \n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "    if cuda: # si on fait du GPU, passage en CUDA\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # On récupère les données\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "\n",
        "    # On itère sur les epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Phase de train\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        # Phase d'evaluation\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        # plot\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXoiJsO0PI5C"
      },
      "source": [
        "main(128, 0.1, cuda=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}